{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m jupytools export -o ../loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import sys\n",
    "from collections import OrderedDict\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "\n",
    "from loop import callbacks as C\n",
    "from loop.config import defaults\n",
    "from loop.metrics import accuracy\n",
    "from loop.modules import TinyNet\n",
    "from loop.phase import Phase\n",
    "from loop.utils import unwrap_if_single"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def default_optimizer(model, **params):\n",
    "    if 'lr' not in params:\n",
    "        params['lr'] = 0.001\n",
    "    return optim.Adam(model.parameters(), **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def create_callbacks(cbs, default: bool=True):\n",
    "    defaults = [C.RollingLoss(), C.History(), C.StreamLogger()] if default else []\n",
    "    cbs = list(cbs or [])\n",
    "    cbs += defaults\n",
    "    return C.Group(cbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "_err_stream = sys.stderr\n",
    "def report_error(exc):\n",
    "    import traceback\n",
    "    tb = traceback.format_tb(exc.__traceback__)\n",
    "    _err_stream.write('Error! Training loop was interupted with un-expected exception\\n')\n",
    "    _err_stream.write('--------------------------------------------------------------\\n')\n",
    "    _err_stream.write('\\n'.join(tb))\n",
    "    _err_stream.write('\\n' + str(exc) + '\\n')\n",
    "    _err_stream.write('--------------------------------------------------------------\\n')\n",
    "    _err_stream.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "_out_stream = sys.stdout\n",
    "def get_output_stream():\n",
    "    return _out_stream\n",
    "def set_output_stream(stream):\n",
    "    _out_stream = stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def write_output(message: str):\n",
    "    _out_stream.write(message)\n",
    "    _out_stream.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Loop:\n",
    "    \"\"\"A generic training loop implementation.\n",
    "    \n",
    "    The class wraps model, phases, optimizer and callbacks, and provides a couple of \n",
    "    methods to make the training process launching a bit more convenient.\n",
    "    \"\"\"\n",
    "    def __init__(self, model: nn.Module, cbs: list=None,\n",
    "                 default_cb: bool=True, opt: 'optimizer'=None, \n",
    "                 opt_fn: 'callable'=default_optimizer, opt_params: dict=None,\n",
    "                 device: 'device'=defaults.device, features_key: str='features',\n",
    "                 targets_key: str='targets', loss_fn: 'callable'=defaults.loss_fn):\n",
    "        \n",
    "        model.to(device)\n",
    "        if opt is None:\n",
    "            opt = opt_fn(model, **(opt_params or {}))\n",
    "\n",
    "        cb = create_callbacks(cbs, default_cb)\n",
    "        cb.loop = self\n",
    "\n",
    "        self.model = model\n",
    "        self.opt = opt\n",
    "        self.cb = cb\n",
    "        self.loss_fn = loss_fn \n",
    "        self.device = device\n",
    "        self.features_key = features_key\n",
    "        self.targets_key = targets_key\n",
    "        \n",
    "    def fit_datasets(self, trn_ds: Dataset, val_ds: Dataset, \n",
    "                     epochs: int=1, batch_size: int=defaults.batch_size):\n",
    "        \"\"\"Uses two torch datasets (training and validation) to fit the model.\"\"\"\n",
    "        phases = Phase.make_train_valid(\n",
    "            trn_ds, val_ds, bs=batch_size,\n",
    "            num_workers=defaults.num_workers)\n",
    "        self.train(phases, epochs)\n",
    "        \n",
    "    def fit_loaders(self, loaders: OrderedDict, epochs: int=1):\n",
    "        \"\"\"Uses dictionary of loaders to create training phases to fit the model.\"\"\"\n",
    "        phases = [\n",
    "            Phase(name=name, loader=loader, grad=(name == 'train'))\n",
    "            for name, loader in loaders.items()]\n",
    "        self.train(phases, epochs)\n",
    "        \n",
    "    def train(self, phases: list, epochs: int=1):\n",
    "        \"\"\"Uses a list of training phases to fit the model.\"\"\"\n",
    "        try:\n",
    "            self.cb.training_started(phases=phases, epochs=epochs)\n",
    "            for epoch in range(1, epochs + 1):\n",
    "                self.train_one_epoch(phases, epoch)\n",
    "            self.cb.training_ended(phases=phases)\n",
    "            \n",
    "        except TrainingInterrupted as e:\n",
    "            self.cb.training_ended(phases=phases)\n",
    "            self.cb.interrupted(exc=e)\n",
    "            \n",
    "        except Exception as e:\n",
    "            report_error(e)\n",
    "            \n",
    "        finally:\n",
    "            self.cb.cleanup()\n",
    "    \n",
    "    def train_one_epoch(self, phases: list, curr_epoch: int=1):\n",
    "        \"\"\"Performs a single training iteration.\"\"\"\n",
    "        cb, model, opt = self.cb, self.model, self.opt\n",
    "\n",
    "        phases = Phase.as_named_list(phases)\n",
    "        \n",
    "        cb.epoch_started(epoch=curr_epoch)\n",
    "\n",
    "        for phase in phases:\n",
    "            n = len(phase.loader)\n",
    "            cb.phase_started(phase=phase, total_batches=n)\n",
    "            is_training = phase.grad\n",
    "            model.train(is_training)\n",
    "            \n",
    "            for batch_no, batch in enumerate(phase.loader):\n",
    "                phase.batch_index += 1\n",
    "                cb.batch_started(phase=phase, total_batches=n)\n",
    "                x, y = to_xy(batch, self.device)\n",
    "                \n",
    "                with torch.set_grad_enabled(is_training):\n",
    "                    cb.before_forward(x=x, y=y)\n",
    "                    out = model(x)\n",
    "                    cb.after_forward(out=out)\n",
    "                    loss = self.loss_fn(out, y)\n",
    "                    cb.after_loss(loss=loss, out=out, target=y)\n",
    "                \n",
    "                if is_training:\n",
    "                    opt.zero_grad() # move into callback and call `before_backward`\n",
    "                    cb.before_backward(phase=phase, batch_no=batch_no)\n",
    "                    loss.backward()\n",
    "                    opt.step()  # move into callback and call `after_backward`\n",
    "                    cb.after_backward(phase=phase, batch_no=batch_no)\n",
    "                \n",
    "                phase.batch_loss = loss.item()\n",
    "                cb.batch_ended(phase=phase, output=out, target=y)\n",
    "                \n",
    "            cb.phase_ended(phase=phase)\n",
    "                    \n",
    "        cb.epoch_ended(phases=phases, epoch=curr_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class TrainingInterrupted(Exception):\n",
    "    \"\"\"Exception which is raised in case if training loop is interrupted.\n",
    "    \n",
    "    Note that this exception is intended to 'gracefully' stop a loop and can be raised from a\n",
    "    callback if it 'decides' that the training should be stopped (e.g. early stopping). All\n",
    "    other types of exceptions are treated as errors and can't guarantee that the loop is in a\n",
    "    consistent state.\n",
    "    \"\"\"\n",
    "    def __init__(self, context=None):\n",
    "        self.context = context\n",
    "    def __str__(self):\n",
    "        return str(self.context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def raise_interruption(context):\n",
    "    raise TrainingInterrupted(context=context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def to_xy(batch, device, features_key='features', targets_key='targets'):\n",
    "    \"\"\"Converts batch object into (x, y) tuple of samples and targets.\n",
    "    \n",
    "    A batch could be one of the following:\n",
    "        * tuple with two arrays X and y of the same size\n",
    "        * dictionary with keys `features_key` and `targets_key`\n",
    "        \n",
    "    \"\"\"\n",
    "    if isinstance(batch, (tuple, list)):\n",
    "        return place_and_unwrap(batch, device)\n",
    "    elif isinstance(batch, (dict, OrderedDict)):\n",
    "        x = batch[features_key]\n",
    "        y = batch[targets_key]\n",
    "        return place_and_unwrap((x, y), device)\n",
    "    raise NotImplementedError(f'unknown batch type: {type(batch)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def place_and_unwrap(batch, device):\n",
    "    \"\"\"Places tensors from batch onto proper device and converts targets\n",
    "    into proper shape depending on number of tensors.\n",
    "    \"\"\"\n",
    "    batch = [t.to(device) for t in batch]\n",
    "    x, *ys = batch\n",
    "    return x, unwrap_if_single(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "from loop.testing import get_mnist\n",
    "loop = Loop(TinyNet(1), cbs=[C.Average(accuracy)], loss_fn=F.cross_entropy)\n",
    "loop.fit_datasets(*get_mnist(), epochs=3, batch_size=2048)\n",
    "loop.cb['history'].plot();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastai (cuda 10)",
   "language": "python",
   "name": "fastai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
