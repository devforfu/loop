{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from collections import OrderedDict\n",
    "from typing import Union, Tuple\n",
    "from enum import IntEnum\n",
    "from operator import itemgetter\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from loop.config import defaults\n",
    "from loop.utils import merge_dicts, to_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Phase:\n",
    "    \"\"\"\n",
    "    Model training loop phase.\n",
    "\n",
    "    Each model's training loop iteration could be separated into (at least) two\n",
    "    phases: training and validation. The instances of this class track\n",
    "    metrics and counters, related to the specific phase, and keep the reference\n",
    "    to subset of data, used during phase.\n",
    "    \"\"\"\n",
    "    def __init__(self, name: str, loader: 'DataLoader', grad: bool=True):\n",
    "        self.name = name\n",
    "        self.loader = loader\n",
    "        self.grad = grad\n",
    "        self.batch_loss = None\n",
    "        self.batch_index = 0\n",
    "        self.rolling_loss = 0\n",
    "        self.losses = []\n",
    "        self.metrics = OrderedDict()\n",
    "\n",
    "    @property\n",
    "    def last_loss(self):\n",
    "        return self.losses[-1] if self.losses else None\n",
    "\n",
    "    @property\n",
    "    def last_metrics(self):\n",
    "        metrics = OrderedDict()\n",
    "        metrics[f'{self.name}_loss'] = self.last_loss\n",
    "        for name, values in self.metrics.items():\n",
    "            metrics[f'{self.name}_{name}'] = values[-1]\n",
    "        return metrics\n",
    "\n",
    "    @property\n",
    "    def metrics_history(self):\n",
    "        metrics = OrderedDict()\n",
    "        for name, values in self.metrics.items():\n",
    "            metrics[f'{self.name}_{name}'] = values\n",
    "        return metrics\n",
    "\n",
    "    def update(self, loss):\n",
    "        self.losses.append(loss)\n",
    "\n",
    "    def update_metric(self, name, value):\n",
    "        if name not in self.metrics:\n",
    "            self.metrics[name] = []\n",
    "        self.metrics[name].append(value)\n",
    "        \n",
    "    @staticmethod\n",
    "    def make_train_valid(trn_ds, val_ds, \n",
    "                         bs: int=defaults.bs,\n",
    "                         num_workers: Union[Tuple, int]=0):\n",
    "        \"\"\"Creates two loop's phases, train and valid.\n",
    "        \n",
    "        The phases are thin wrappers on top of data loaders intended to track\n",
    "        additional information gathered during model's fitting process, like \n",
    "        loss, performance metrics, etc.\n",
    "        \"\"\"\n",
    "        trn, val = unwrap(num_workers, 2)\n",
    "        phs = OrderedDict()\n",
    "        phs['train'] = Phase('train', DataLoader(trn_ds, bs, shuffle=True, num_workers=trn))\n",
    "        phs['valid'] = Phase('valid', DataLoader(val_ds, bs, num_workers=val), grad=False)\n",
    "        return phs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mock_loader(): return DataLoader(TensorDataset(torch.randn((1000, 10))))\n",
    "train = Phase('train', mock_loader())\n",
    "assert train.name == 'train'\n",
    "assert train.loader is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def is_scalar(obj):\n",
    "    return isinstance(obj, (int, float, str, complex))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def unwrap(obj, pad=1):\n",
    "    \"\"\"Convenience function to unwrap collections and broadcast scalars.\"\"\"\n",
    "    if is_scalar(obj): \n",
    "        return [obj]*pad\n",
    "    return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = defaults.datasets/'mnist'\n",
    "trn_ds = MNIST(root, train=True)\n",
    "val_ds = MNIST(root, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "phases = Phase.make_train_valid(trn_ds, val_ds)\n",
    "assert len(phases) == 2\n",
    "assert phases['train'].loader is not None\n",
    "assert phases['valid'].loader is not None\n",
    "assert phases['train'].grad \n",
    "assert not phases['valid'].grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Callback:\n",
    "    \"\"\"The base class inherited by callbacks.\n",
    "    \n",
    "    Provides a lot of hooks invoked on various stages of the training loop\n",
    "    execution. The signature of functions is as broad as possible to allow\n",
    "    flexibility and customization in descendant classes.\n",
    "    \"\"\"\n",
    "    def training_started(self, **kwargs): pass\n",
    "    def training_ended(self, **kwargs): pass\n",
    "    def epoch_started(self, **kwargs): pass\n",
    "    def epoch_ended(self, **kwargs): pass\n",
    "    def phase_started(self, **kwargs): pass\n",
    "    def phase_ended(self, **kwargs): pass\n",
    "    def batch_started(self, **kwargs): pass\n",
    "    def batch_ended(self, **kwargs): pass\n",
    "    def before_forward(self, **kwargs): pass\n",
    "    def after_forward(self, **kwargs): pass\n",
    "    def before_backward(self, **kwargs): pass\n",
    "    def after_backward(self, **kwargs): pass\n",
    "    def interrupted(self, **kwargs): pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import IntFlag\n",
    "\n",
    "class Order(IntFlag):\n",
    "    Unknown = -1\n",
    "    Internal = 0\n",
    "    Loss = 10\n",
    "    Metrics = 100\n",
    "    Schedule = 200\n",
    "    History = 300\n",
    "    Logging = 1000\n",
    "    \n",
    "    def __call__(self, index=0):\n",
    "        return Order(self.value + index)\n",
    "    \n",
    "    @staticmethod\n",
    "    def sort(items):\n",
    "        ordered = [(getattr(item, Order.Unknown), item) for item in items]\n",
    "        ordered.sort(key=itemgetter(0))\n",
    "        return [item for _, item in ordered]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class RollingLoss(Callback):\n",
    "    \"\"\"A callback that tracks model's loss.\n",
    "    \n",
    "    The loss is interpolated between current and next value to get \n",
    "    a smoother loss curve.\n",
    "    \"\"\"\n",
    "    order = Order.Loss()\n",
    "    \n",
    "    def __init__(self, smooth=0.98):\n",
    "        self.smooth = smooth\n",
    "    \n",
    "    def batch_ended(self, phase, **kwargs):\n",
    "        prev = phase.rolling_loss\n",
    "        a = self.smooth\n",
    "        avg_loss = a*prev + (1 - a)*phase.batch_loss\n",
    "        debias_loss = avg_loss / (1 - a**phase.batch_index)\n",
    "        phase.rolling_loss = avg_loss\n",
    "        phase.update(debias_loss)\n",
    "        \n",
    "    def epoch_ended(self, phases, **kwargs):\n",
    "        for phase in phases:\n",
    "            phase.update_metric('loss', phase.last_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class History(Callback):\n",
    "    \"\"\"A callback that collects model's metrics during its training.\"\"\"\n",
    "    \n",
    "    order = Order.History()\n",
    "    \n",
    "    def training_started(self, **kwargs):\n",
    "        self.recorded = None\n",
    "        self.epochs = []\n",
    "    \n",
    "    def epoch_ended(self, epoch, **kwargs):\n",
    "        self.epochs.append(epoch)\n",
    "        \n",
    "    def training_ended(self, phases, **kwargs):\n",
    "        epochs = {'epoch': np.array(self.epochs).astype(int)}\n",
    "        metrics = [epochs] + [p.metrics_history for p in phases]\n",
    "        data = pd.DataFrame(merge_dicts(metrics))\n",
    "        data.reset_index(inplace=True, drop=True)\n",
    "        self.recorded = data\n",
    "        \n",
    "    def plot(self, x='epoch', ys='loss', ax=None):\n",
    "        return self.recorded.plot(x='epoch', y=to_list(ys), ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Group(Callback):\n",
    "    def __init__(self, cbs):\n",
    "        self._init(self, cbs)\n",
    "        self._model = None\n",
    "        \n",
    "    def _init(self, cbs):\n",
    "        if not cbs:\n",
    "            self.callbacks = []\n",
    "            self.named_callbacks = {}\n",
    "        else:\n",
    "            cbs = Order.sort(cbs)\n",
    "            for cb in cbs:\n",
    "                cb.group = self\n",
    "            self.callbacks = cbs\n",
    "            self.named_callbacks = {to_snake_case(classname(cb)): cb for cb in cbs}\n",
    "        \n",
    "    def add(self, cb, *cbs):\n",
    "        cbs = [cb] + list(cbs)\n",
    "        self._init(cbs)\n",
    "        \n",
    "    def set_model(self, model):\n",
    "        self._model = model\n",
    "\n",
    "    def training_started(self, **kwargs): self('training_started', **kwargs)    \n",
    "    def training_ended(self, **kwargs): self('training_ended', **kwargs)\n",
    "    def epoch_started(self, **kwargs): self('epoch_started', **kwargs)\n",
    "    def epoch_ended(self, **kwargs): self('epoch_ended', **kwargs)\n",
    "    def phase_started(self, **kwargs): self('phase_started', **kwargs)\n",
    "    def phase_ended(self, **kwargs): self('phase_ended', **kwargs)\n",
    "    def batch_started(self, **kwargs): self('batch_started', **kwargs)\n",
    "    def batch_ended(self, **kwargs): self('batch_ended', **kwargs)\n",
    "    def before_forward(self, **kwargs): self('before_forward', **kwargs)\n",
    "    def after_forward(self, **kwargs): self('after_forward', **kwargs)\n",
    "    def before_backward(self, **kwargs): self('before_forward', **kwargs)\n",
    "    def after_backward(self, **kwargs): self('after_backward', **kwargs)\n",
    "    def interrupted(self, **kwargs): self('interrupted', **kwargs)\n",
    "        \n",
    "    def __getattr__(self, item):\n",
    "        if item in vars(self):\n",
    "            return self.__dict__[item]\n",
    "        if self._model is not None:\n",
    "            return getattr(self._model)\n",
    "        raise AttributeError(item)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        item = to_snake_case(item)\n",
    "        if item in self.named_callbacks:\n",
    "            return self.named_callbacks[item]\n",
    "        raise KeyError(\n",
    "            f'callback name is not found: {item}; '\n",
    "            f'available callbacks are: {list(sorted(self.named_callbacks))}')\n",
    "    \n",
    "    def __call__(self, name, **kwargs):\n",
    "        for cb in self.callbacks:\n",
    "            method = getattr(cb, name, None)\n",
    "            if method is None:\n",
    "                continue\n",
    "            method(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def default_optimizer(model, **params):\n",
    "    if 'lr' not in params:\n",
    "        params['lr'] = 0.001\n",
    "    return optim.Adam(model.parameters(), **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_callbacks(cbs, default: bool=True):\n",
    "    if cbs is None:\n",
    "        cbs = [RollingLoss(), History()] if default else []\n",
    "    else:\n",
    "        cbs = list(cbs)\n",
    "        if default:\n",
    "            cbs += [RollingLoss(), History()]\n",
    "    return Group(cbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def place_and_unwrap(batch, dev):\n",
    "    \"\"\"Places tensors from batch onto proper device and \n",
    "    returns (x, y) pair where all target tensors (if many)\n",
    "    are put into y list.\n",
    "    \"\"\"\n",
    "    batch = [t.to(dev) for t in batch]\n",
    "    x, *ys = batch\n",
    "    return x, to_list(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingInterrupted(Exception):\n",
    "    def __init__(self, context=None):\n",
    "        self.context = context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loop:\n",
    "    def __init__(self, model: nn.Module,\n",
    "                 default_cb: bool=True, opt_fn: 'callable'=default_optimizer,\n",
    "                 opt_params=None, device: 'device'=defaults.device,\n",
    "                 loss_fn=defaults.loss_fn):\n",
    "        \n",
    "        breakpoint()\n",
    "        model.to(device)\n",
    "        opt = opt_fn(model, **(opt_params or {}))\n",
    "        cb = create_callbacks(cbs, default_cb)\n",
    "        cb.set_model(model)\n",
    "        \n",
    "        self.model = model\n",
    "        self.opt = opt\n",
    "        self.cb = cb\n",
    "        self.loss_fn = loss_fn \n",
    "        self.device = device\n",
    "        \n",
    "    def fit_datasets(trn_ds, val_ds, epochs: int=1, batch_size: int=defaults.bs):\n",
    "        phases = Phase.make_train_valid(\n",
    "            trn_ds, val_ds, bs=batch_size, num_workers=defaults.n_jobs)\n",
    "        self.train(phases, epochs)\n",
    "        \n",
    "    def train(phases: list, epochs: int=1):\n",
    "        try:\n",
    "            self.cb.training_started(phases=phases)\n",
    "            for epoch in range(1, epochs + 1):\n",
    "                self.train_one_epoch(phases, epoch)\n",
    "            self.cb.training_ended(phases=phases)\n",
    "        except TrainingInterrupted as e:\n",
    "            self.cb.interrupted(reason=e)\n",
    "    \n",
    "    def train_one_epoch(phases: list, curr_epoch: int=1):\n",
    "        self.cb.epoch_started(epoch=curr_epoch)\n",
    "\n",
    "        for phase in phases:\n",
    "            n = len(phase.loader)\n",
    "            cb.phase_started(phase=phase, total_batches=n)\n",
    "            is_training = phase.grad\n",
    "            model.train(is_training)\n",
    "            \n",
    "            for batch in phase.loader:\n",
    "                phase.batch_index += 1\n",
    "                cb.batch_started(phase=phase, total_batches=n)\n",
    "                x, y = place_and_unwrap(batch, self.device)\n",
    "                \n",
    "                with torch.set_grad_enabled(is_training):\n",
    "                    cb.before_forward()\n",
    "                    out = model(x)\n",
    "                    cb.after_forward()\n",
    "                    loss = self.loss_fn(out, y)\n",
    "                \n",
    "                if is_training:\n",
    "                    opt.zero_grad()\n",
    "                    cb.before_backward()\n",
    "                    loss.backward()\n",
    "                    opt.step()\n",
    "                \n",
    "                phase.batch_loss = loss.item()\n",
    "                cb.batch_ended(phase=phase, output=out, target=y)\n",
    "                \n",
    "            cb.phase_ended(phase=phase)\n",
    "                    \n",
    "        self.cb.epoch_ended(phases=phases, epoch=curr_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class TinyNet(nn.Module):\n",
    "    def __init__(self, n_out=10):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(28, 64, 3)\n",
    "        self.conv2 = nn.Conv2d(64, 128, 3)\n",
    "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc1 = nn.Linear(128, 64)\n",
    "        self.fc2 = nn.Linear(64, n_out)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return self.fc2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> <ipython-input-56-d2b2b96ab060>(8)__init__()\n",
      "-> model.to(device)\n",
      "(Pdb) device\n"
     ]
    }
   ],
   "source": [
    "loop = Loop(TinyNet(), loss_fn=F.cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastai (cuda 10)",
   "language": "python",
   "name": "fastai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
