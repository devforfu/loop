{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from collections import defaultdict, OrderedDict\n",
    "from enum import IntFlag\n",
    "from operator import itemgetter\n",
    "import sys\n",
    "from typing import Union, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from loop.config import defaults\n",
    "from loop.utils import merge_dicts, to_list, to_snake_case, classname, autoformat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NamedList:\n",
    "    def __init__(self, od: OrderedDict):\n",
    "        self.od = od\n",
    "        self.names = list(od)\n",
    "        \n",
    "    def __iter__(self):\n",
    "        self.index = -1\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        self.index += 1\n",
    "        if self.index >= len(self):\n",
    "            raise StopIteration()\n",
    "        return self[self.index]\n",
    "    \n",
    "    def __len__(self): \n",
    "        return len(self.od)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        if isinstance(item, str):\n",
    "            return self.od[item]\n",
    "        elif isinstance(item, int):\n",
    "            return self.od[self.names[item]]\n",
    "        return TypeError(f'invalid index type: {type(item)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Phase:\n",
    "    \"\"\"\n",
    "    Model training loop phase.\n",
    "\n",
    "    Each model's training loop iteration could be separated into (at least) two\n",
    "    phases: training and validation. The instances of this class track\n",
    "    metrics and counters, related to the specific phase, and keep the reference\n",
    "    to subset of data, used during phase.\n",
    "    \"\"\"\n",
    "    def __init__(self, name: str, loader: 'DataLoader', grad: bool=True):\n",
    "        self.name = name\n",
    "        self.loader = loader\n",
    "        self.grad = grad\n",
    "        self.batch_loss = None\n",
    "        self.batch_index = 0\n",
    "        self.rolling_loss = 0\n",
    "        self.losses = []\n",
    "        self.metrics = OrderedDict()\n",
    "\n",
    "    @property\n",
    "    def last_loss(self):\n",
    "        return self.losses[-1] if self.losses else None\n",
    "\n",
    "    @property\n",
    "    def last_metrics(self):\n",
    "        metrics = OrderedDict()\n",
    "        metrics[f'{self.name}_loss'] = self.last_loss\n",
    "        for name, values in self.metrics.items():\n",
    "            metrics[f'{self.name}_{name}'] = values[-1]\n",
    "        return metrics\n",
    "\n",
    "    @property\n",
    "    def metrics_history(self):\n",
    "        metrics = OrderedDict()\n",
    "        for name, values in self.metrics.items():\n",
    "            metrics[f'{self.name}_{name}'] = values\n",
    "        return metrics\n",
    "\n",
    "    def update(self, loss):\n",
    "        self.losses.append(loss)\n",
    "\n",
    "    def update_metric(self, name, value):\n",
    "        if name not in self.metrics:\n",
    "            self.metrics[name] = []\n",
    "        self.metrics[name].append(value)\n",
    "        \n",
    "    @staticmethod\n",
    "    def make_train_valid(trn_ds, val_ds, \n",
    "                         bs: int=defaults.bs,\n",
    "                         num_workers: Union[Tuple, int]=0):\n",
    "        \"\"\"Creates two loop's phases, train and valid.\n",
    "        \n",
    "        The phases are thin wrappers on top of data loaders intended to track\n",
    "        additional information gathered during model's fitting process, like \n",
    "        loss, performance metrics, etc.\n",
    "        \"\"\"\n",
    "        trn, val = broadcast(num_workers, 2)\n",
    "        phs = OrderedDict()\n",
    "        phs['train'] = Phase('train', DataLoader(trn_ds, bs, shuffle=True, num_workers=trn))\n",
    "        phs['valid'] = Phase('valid', DataLoader(val_ds, bs, num_workers=val), grad=False)\n",
    "        return NamedList(phs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "od = OrderedDict()\n",
    "od['first'] = 1\n",
    "od['last'] = 2\n",
    "nl = NamedList(od)\n",
    "assert len(nl) == len(od)\n",
    "assert [nl['first'], nl['last']] == list(nl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mock_loader(): return DataLoader(TensorDataset(torch.randn((1000, 10))))\n",
    "train = Phase('train', mock_loader())\n",
    "assert train.name == 'train'\n",
    "assert train.loader is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = defaults.datasets/'mnist'\n",
    "trn_ds = MNIST(root, train=True)\n",
    "val_ds = MNIST(root, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "phases = Phase.make_train_valid(trn_ds, val_ds)\n",
    "assert len(phases) == 2\n",
    "assert phases['train'].loader is not None\n",
    "assert phases['valid'].loader is not None\n",
    "assert phases['train'].grad \n",
    "assert not phases['valid'].grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Callback:\n",
    "    \"\"\"The base class inherited by callbacks.\n",
    "    \n",
    "    Provides a lot of hooks invoked on various stages of the training loop\n",
    "    execution. The signature of functions is as broad as possible to allow\n",
    "    flexibility and customization in descendant classes.\n",
    "    \"\"\"\n",
    "    def training_started(self, **kwargs): pass\n",
    "    def training_ended(self, **kwargs): pass\n",
    "    def epoch_started(self, **kwargs): pass\n",
    "    def epoch_ended(self, **kwargs): pass\n",
    "    def phase_started(self, **kwargs): pass\n",
    "    def phase_ended(self, **kwargs): pass\n",
    "    def batch_started(self, **kwargs): pass\n",
    "    def batch_ended(self, **kwargs): pass\n",
    "    def before_forward(self, **kwargs): pass\n",
    "    def after_forward(self, **kwargs): pass\n",
    "    def before_backward(self, **kwargs): pass\n",
    "    def after_backward(self, **kwargs): pass\n",
    "    def interrupted(self, **kwargs): pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Order(IntFlag):\n",
    "    Unknown = -1\n",
    "    Internal = 0\n",
    "    Loss = 10\n",
    "    Metrics = 100\n",
    "    Schedule = 200\n",
    "    History = 300\n",
    "    Logging = 1000\n",
    "    \n",
    "    def __call__(self, index=0):\n",
    "        return Order(self.value + index)\n",
    "    \n",
    "    @staticmethod\n",
    "    def sort(items):\n",
    "        ordered = [(getattr(item, 'order', Order.Unknown), item) for item in items]\n",
    "        ordered.sort(key=itemgetter(0))\n",
    "        return [item for _, item in ordered]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class RollingLoss(Callback):\n",
    "    \"\"\"A callback that tracks model's loss.\n",
    "    \n",
    "    The loss is interpolated between current and next value to get \n",
    "    a smoother loss curve.\n",
    "    \"\"\"\n",
    "    order = Order.Loss()\n",
    "    \n",
    "    def __init__(self, smooth=0.98):\n",
    "        self.smooth = smooth\n",
    "    \n",
    "    def batch_ended(self, phase, **kwargs):\n",
    "        prev = phase.rolling_loss\n",
    "        a = self.smooth\n",
    "        avg_loss = a*prev + (1 - a)*phase.batch_loss\n",
    "        debias_loss = avg_loss / (1 - a**phase.batch_index)\n",
    "        phase.rolling_loss = avg_loss\n",
    "        phase.update(debias_loss)\n",
    "        \n",
    "    def epoch_ended(self, phases, **kwargs):\n",
    "        for phase in phases:\n",
    "            phase.update_metric('loss', phase.last_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class History(Callback):\n",
    "    \"\"\"A callback that collects model's metrics during its training.\"\"\"\n",
    "    \n",
    "    order = Order.History()\n",
    "    \n",
    "    def training_started(self, **kwargs):\n",
    "        self.recorded = None\n",
    "        self.epochs = []\n",
    "    \n",
    "    def epoch_ended(self, epoch, **kwargs):\n",
    "        self.epochs.append(epoch)\n",
    "        \n",
    "    def training_ended(self, phases, **kwargs):\n",
    "        epochs = {'epoch': np.array(self.epochs).astype(int)}\n",
    "        metrics = [epochs] + [p.metrics_history for p in phases]\n",
    "        data = pd.DataFrame(merge_dicts(metrics))\n",
    "        data.reset_index(inplace=True, drop=True)\n",
    "        self.recorded = data\n",
    "        \n",
    "    def plot(self, x='epoch', ys='loss', ax=None):\n",
    "        return self.recorded.plot(x='epoch', y=to_list(ys), ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autoformat(v):\n",
    "    \"\"\"Tryies to convert value into a string using the best possible representation.\"\"\"\n",
    "    \n",
    "    return (f'{v:d}' if isinstance(v, (int, np.int16, np.int32, np.int64)) else\n",
    "            f'{v:.4f}' if isinstance(v, (float, np.float16, np.float32, np.float64)) else\n",
    "            f'{str(v)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StreamLogger(Callback):\n",
    "    \"\"\"\n",
    "    Writes performance metrics collected during the training process into list\n",
    "    of streams.\n",
    "    \n",
    "    Parameters:\n",
    "        streams: A list of file-like objects with `write()` method.\n",
    "    \n",
    "    \"\"\"\n",
    "    order = Order.Logging()\n",
    "    \n",
    "    def __init__(self, streams: list=None, log_every: int=1):\n",
    "        self.streams = streams or [sys.stdout]\n",
    "        self.log_every = log_every\n",
    "        \n",
    "    def epoch_ended(self, phases, epoch, **kwargs):\n",
    "        metrics = merge_dicts([p.last_metrics for p in phases])\n",
    "        values = [f'{k}={autoformat(v)}' for k, v in metrics.items()]\n",
    "        values_string = ', '.join(values)\n",
    "        string = f'Epoch: {epoch:4d} | {values_string}\\n'\n",
    "        for stream in self.streams:\n",
    "            stream.write(string)\n",
    "            stream.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(out, y_true):\n",
    "    y_hat = out.argmax(dim=-1).view(y_true.size(0), -1)\n",
    "    y_true = y_true.view(y_true.size(0), -1)\n",
    "    match = y_hat == y_true\n",
    "    return match.float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def from_torch(tensor):\n",
    "    obj = tensor.detach().cpu()\n",
    "    if not obj.shape:\n",
    "        return obj.item()\n",
    "    return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Average(Callback):\n",
    "    \n",
    "    def __init__(self, metric_fn: 'callable', name: str=None):\n",
    "        self.metric_fn = metric_fn\n",
    "        self.name = name or self.metric_fn.__name__\n",
    "    \n",
    "    def epoch_started(self, **kwargs):\n",
    "        self.values = defaultdict(int)\n",
    "        self.counts = defaultdict(int)\n",
    "        \n",
    "    def batch_ended(self, phase, output, target, **kwargs):\n",
    "        metric = from_torch(self.metric_fn(output, target))\n",
    "        self.counts[phase.name] += target.size(0)\n",
    "        self.values[phase.name] += target.size(0) * metric\n",
    "        \n",
    "    def epoch_ended(self, phases, **kwargs):\n",
    "        for phase in phases:\n",
    "            metric = self.values[phase.name] / self.counts[phase.name]\n",
    "            phase.update_metric(self.name, metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Group(Callback):\n",
    "    def __init__(self, cbs):\n",
    "        self._init(cbs)\n",
    "        self._model = None\n",
    "        \n",
    "    def _init(self, cbs):\n",
    "        if not cbs:\n",
    "            self.callbacks = []\n",
    "            self.named_callbacks = {}\n",
    "        else:\n",
    "            cbs = Order.sort(cbs)\n",
    "            for cb in cbs:\n",
    "                cb.group = self\n",
    "            self.callbacks = cbs\n",
    "            self.named_callbacks = {to_snake_case(classname(cb)): cb for cb in cbs}\n",
    "        \n",
    "    def add(self, cb, *cbs):\n",
    "        cbs = [cb] + list(cbs)\n",
    "        self._init(cbs)\n",
    "        \n",
    "    def set_model(self, model):\n",
    "        self._model = model\n",
    "\n",
    "    def training_started(self, **kwargs): self('training_started', **kwargs)    \n",
    "    def training_ended(self, **kwargs): self('training_ended', **kwargs)\n",
    "    def epoch_started(self, **kwargs): self('epoch_started', **kwargs)\n",
    "    def epoch_ended(self, **kwargs): self('epoch_ended', **kwargs)\n",
    "    def phase_started(self, **kwargs): self('phase_started', **kwargs)\n",
    "    def phase_ended(self, **kwargs): self('phase_ended', **kwargs)\n",
    "    def batch_started(self, **kwargs): self('batch_started', **kwargs)\n",
    "    def batch_ended(self, **kwargs): self('batch_ended', **kwargs)\n",
    "    def before_forward(self, **kwargs): self('before_forward', **kwargs)\n",
    "    def after_forward(self, **kwargs): self('after_forward', **kwargs)\n",
    "    def before_backward(self, **kwargs): self('before_forward', **kwargs)\n",
    "    def after_backward(self, **kwargs): self('after_backward', **kwargs)\n",
    "    def interrupted(self, **kwargs): self('interrupted', **kwargs)\n",
    "        \n",
    "    def __getattr__(self, item):\n",
    "        if item in vars(self):\n",
    "            return self.__dict__[item]\n",
    "        if self._model is not None:\n",
    "            return getattr(self._model, item)\n",
    "        raise AttributeError(item)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        item = to_snake_case(item)\n",
    "        if item in self.named_callbacks:\n",
    "            return self.named_callbacks[item]\n",
    "        raise KeyError(\n",
    "            f'callback name is not found: {item}; '\n",
    "            f'available callbacks are: {list(sorted(self.named_callbacks))}')\n",
    "    \n",
    "    def __call__(self, name, **kwargs):\n",
    "        for cb in self.callbacks:\n",
    "            method = getattr(cb, name, None)\n",
    "            if method is None:\n",
    "                continue\n",
    "            method(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def default_optimizer(model, **params):\n",
    "    if 'lr' not in params:\n",
    "        params['lr'] = 0.001\n",
    "    return optim.Adam(model.parameters(), **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def create_callbacks(cbs, default: bool=True):\n",
    "    defaults = [RollingLoss(), History(), StreamLogger()] if default else []\n",
    "    cbs = list(cbs or [])\n",
    "    cbs += defaults\n",
    "    return Group(cbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def unwrap_if_single(obj):\n",
    "    \"\"\"Converts obj collection into scalar if it contains single element only.\"\"\"\n",
    "    return obj[0] if len(obj) == 1 else obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def place_and_unwrap(batch, dev):\n",
    "    \"\"\"Places tensors from batch onto proper device and converts targets\n",
    "    into proper shape depending on number of tensors.\n",
    "    \"\"\"\n",
    "    batch = [t.to(dev) for t in batch]\n",
    "    x, *ys = batch\n",
    "    return x, unwrap_if_single(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class TrainingInterrupted(Exception):\n",
    "    def __init__(self, context=None):\n",
    "        self.context = context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Loop:\n",
    "    def __init__(self, model: nn.Module, cbs: list=None,\n",
    "                 default_cb: bool=True, opt_fn: 'callable'=default_optimizer,\n",
    "                 opt_params: dict=None, device: 'device'=defaults.device,\n",
    "                 loss_fn: 'callable'=defaults.loss_fn):\n",
    "        \n",
    "        model.to(device)\n",
    "        opt = opt_fn(model, **(opt_params or {}))\n",
    "        cb = create_callbacks(cbs, default_cb)\n",
    "        cb.set_model(model)\n",
    "        \n",
    "        self.model = model\n",
    "        self.opt = opt\n",
    "        self.cb = cb\n",
    "        self.loss_fn = loss_fn \n",
    "        self.device = device\n",
    "        \n",
    "    def fit_datasets(self, trn_ds, val_ds, epochs: int=1, batch_size: int=defaults.bs):\n",
    "        phases = Phase.make_train_valid(\n",
    "            trn_ds, val_ds, bs=batch_size, num_workers=defaults.n_jobs)\n",
    "        self.train(phases, epochs)\n",
    "        \n",
    "    def train(self, phases: list, epochs: int=1):\n",
    "        try:\n",
    "            self.cb.training_started(phases=phases)\n",
    "            for epoch in range(1, epochs + 1):\n",
    "                self.train_one_epoch(phases, epoch)\n",
    "            self.cb.training_ended(phases=phases)\n",
    "        except TrainingInterrupted as e:\n",
    "            self.cb.interrupted(reason=e)\n",
    "    \n",
    "    def train_one_epoch(self, phases: list, curr_epoch: int=1):\n",
    "        cb, model, opt = self.cb, self.model, self.opt\n",
    "        \n",
    "        cb.epoch_started(epoch=curr_epoch)\n",
    "\n",
    "        for phase in phases:\n",
    "            n = len(phase.loader)\n",
    "            cb.phase_started(phase=phase, total_batches=n)\n",
    "            is_training = phase.grad\n",
    "            model.train(is_training)\n",
    "            \n",
    "            for batch in phase.loader:\n",
    "                phase.batch_index += 1\n",
    "                cb.batch_started(phase=phase, total_batches=n)\n",
    "                x, y = place_and_unwrap(batch, self.device)\n",
    "                \n",
    "                with torch.set_grad_enabled(is_training):\n",
    "                    cb.before_forward()\n",
    "                    out = model(x)\n",
    "                    cb.after_forward()\n",
    "                    loss = self.loss_fn(out, y)\n",
    "                \n",
    "                if is_training:\n",
    "                    opt.zero_grad()\n",
    "                    cb.before_backward()\n",
    "                    loss.backward()\n",
    "                    opt.step()\n",
    "                \n",
    "                phase.batch_loss = loss.item()\n",
    "                cb.batch_ended(phase=phase, output=out, target=y)\n",
    "                \n",
    "            cb.phase_ended(phase=phase)\n",
    "                    \n",
    "        cb.epoch_ended(phases=phases, epoch=curr_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class TinyNet(nn.Module):\n",
    "    def __init__(self, n_out=10):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3)\n",
    "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc1 = nn.Linear(64, 64)\n",
    "        self.fc2 = nn.Linear(64, n_out)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return self.fc2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mnist():\n",
    "    root = defaults.datasets/'mnist'\n",
    "\n",
    "    mnist_stats = ([0.15]*1, [0.15]*1)\n",
    "\n",
    "    trn_ds = MNIST(root, train=True, transform=T.Compose([\n",
    "        T.Resize(32),\n",
    "        T.RandomAffine(5, translate=(0.05, 0.05), scale=(0.9, 1.1)),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(*mnist_stats)\n",
    "    ]))\n",
    "    val_ds = MNIST(root, train=False, transform=T.Compose([\n",
    "        T.Resize(32),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(*mnist_stats)\n",
    "    ]))\n",
    "    \n",
    "    return trn_ds, val_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "loop = Loop(TinyNet(), cbs=[Average(accuracy)], loss_fn=F.cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:    1 | train_loss=2.2221, train_accuracy=0.1807, valid_loss=2.0829, valid_accuracy=0.2290\n",
      "Epoch:    2 | train_loss=2.0750, train_accuracy=0.2664, valid_loss=1.9681, valid_accuracy=0.3166\n",
      "Epoch:    3 | train_loss=1.9072, train_accuracy=0.3431, valid_loss=1.8346, valid_accuracy=0.3995\n",
      "Epoch:    4 | train_loss=1.7552, train_accuracy=0.3929, valid_loss=1.7355, valid_accuracy=0.4380\n",
      "Epoch:    5 | train_loss=1.6456, train_accuracy=0.4248, valid_loss=1.6583, valid_accuracy=0.4589\n",
      "Epoch:    6 | train_loss=1.5627, train_accuracy=0.4476, valid_loss=1.5953, valid_accuracy=0.5147\n",
      "Epoch:    7 | train_loss=1.4994, train_accuracy=0.4670, valid_loss=1.5420, valid_accuracy=0.5079\n",
      "Epoch:    8 | train_loss=1.4458, train_accuracy=0.4869, valid_loss=1.4946, valid_accuracy=0.5427\n",
      "Epoch:    9 | train_loss=1.3994, train_accuracy=0.5025, valid_loss=1.4515, valid_accuracy=0.5340\n",
      "Epoch:   10 | train_loss=1.3556, train_accuracy=0.5220, valid_loss=1.4106, valid_accuracy=0.5796\n"
     ]
    }
   ],
   "source": [
    "loop.fit_datasets(*get_mnist(), epochs=10, batch_size=1048)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastai (cuda 10)",
   "language": "python",
   "name": "fastai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
