{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'chunks' from 'loop.utils' (/home/ck/code/loop/dev/loop/utils.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-83-518dbfd6219a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mannotations\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMaybeList\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCallable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcombine\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'chunks' from 'loop.utils' (/home/ck/code/loop/dev/loop/utils.py)"
     ]
    }
   ],
   "source": [
    "#export\n",
    "\"\"\"\n",
    "Text processing utils. Mostly copied from the fastai library.\n",
    "\n",
    "The utilities help to convert \"raw\" texts into formats more suitable for\n",
    "NLP models. The texts are cleaned and converted into list of tokens.\n",
    "\"\"\"\n",
    "import html\n",
    "from multiprocessing import cpu_count\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "\n",
    "from loop.annotations import MaybeList, Callable\n",
    "from loop.utils import combine, chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "SEP    = 'â€¢'\n",
    "T_UP   = 'xxup'\n",
    "T_REP  = 'xxrep'\n",
    "T_WREP = 'xxwrep'\n",
    "T_MAJ  = 'xxmaj'\n",
    "T_BOS  = 'xxbos'\n",
    "T_EOS  = 'xxeos'\n",
    "T_FLD  = 'xxfld'\n",
    "T_UNK  = 'xxunk'\n",
    "T_PAD  = 'xxpad'\n",
    "TOKENS = [T_UP, T_REP, T_WREP, T_MAJ, T_BOS, T_EOS, T_FLD, T_UNK, T_PAD]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def replace_tabs_with_spaces(s: str) -> str: return s.replace('\\t', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert replace_tabs_with_spaces('\\ttabs\\t') == ' tabs '\n",
    "assert replace_tabs_with_spaces('\\t\\t\\tmore tabs\\t\\t\\t') == '   more tabs   '\n",
    "assert replace_tabs_with_spaces('noop') == 'noop'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def add_spaces_around(s: str) -> str: return re.sub(r'([/#\\n])', r' \\1 ', s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert add_spaces_around('#') == ' # '\n",
    "assert add_spaces_around('\\n') == ' \\n '\n",
    "assert add_spaces_around('noop') == 'noop'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def trim_useless_spaces(s: str) -> str: return re.sub(' {2,}', ' ', s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "space = ' '\n",
    "assert all([trim_useless_spaces(space * i) == space for i in range (1, 11)])\n",
    "assert trim_useless_spaces(f'{space}word{space}') == f'{space}word{space}'\n",
    "assert trim_useless_spaces('noop') == 'noop'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def replace_repeated_chars(s: str) -> str:\n",
    "    def _replace(match):\n",
    "        char, repeats = match.groups()\n",
    "        return f' {T_REP} {len(repeats) + 1} {char} '\n",
    "    regex = re.compile(r'(\\S)(\\1{3,})')\n",
    "    return regex.sub(_replace, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert replace_repeated_chars('aaaa') == f' {T_REP} 4 a '\n",
    "assert replace_repeated_chars('sooooo cooool') == f's {T_REP} 5 o  c {T_REP} 4 o l'\n",
    "assert replace_repeated_chars('noop') == 'noop'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def replace_repeated_words(s: str) -> str:\n",
    "    def _replace(match):\n",
    "        word, repeats = match.groups()\n",
    "        return f' {T_WREP} {len(repeats.split()) + 1} {word} '\n",
    "    regex = re.compile(r'(\\b\\w+\\W+)(\\1{3,})')\n",
    "    return regex.sub(_replace, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert replace_repeated_words('one one one one one') == f' {T_WREP} 4 one  one'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def fix_wikitext_special_cases(s: str) -> str:\n",
    "    regex = re.compile(r'  +')\n",
    "    s = (s.\n",
    "         replace('#39;',    \"'\").replace('amp;',    '&').replace('#146;', \"'\").\n",
    "         replace('nbsp;',   ' ').replace('#36;',    '$').replace('\\\\n',  \"\\n\").\n",
    "         replace('quot;',   \"'\").replace('<br />', \"\\n\").replace('\\\\\"',   '\"').\n",
    "         replace(' @.@ ',   '.').replace(' @-@ ',   '-').replace(' @,@ ', ',').\n",
    "         replace('\\\\',   ' \\\\ ').replace('<unk>', T_UNK))\n",
    "    return regex.sub(' ', html.unescape(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def replace_capslock(tokens: list) -> list:\n",
    "    new = []\n",
    "    for token in tokens:\n",
    "        if token.isupper() and len(token) > 1:\n",
    "            new += [T_UP, token.lower()]\n",
    "        else:\n",
    "            new.append(token)\n",
    "    return new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert replace_capslock(['CAPSLOCK']) == [T_UP, 'capslock']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def replace_capitalized(tokens: list) -> list:\n",
    "    new = []\n",
    "    for token in tokens:\n",
    "        if token == '':\n",
    "            continue\n",
    "        if token[0].isupper() and len(token) > 1 and token[1:].islower():\n",
    "            new.append(T_MAJ)\n",
    "        new.append(token.lower())\n",
    "    return new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert replace_capitalized(['Capitalized', 'Words']) == [T_MAJ, 'capitalized', T_MAJ, 'words']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "PREP_RULES = [\n",
    "    replace_tabs_with_spaces,\n",
    "    add_spaces_around,\n",
    "    trim_useless_spaces,\n",
    "    replace_repeated_chars,\n",
    "    replace_repeated_words,\n",
    "    fix_wikitext_special_cases,\n",
    "]\n",
    "\n",
    "POST_RULES = [\n",
    "    replace_capslock,\n",
    "    replace_capitalized\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def tokenize(text: str, prep: MaybeList=None, post: MaybeList=None, \n",
    "             special: MaybeList=None, model_fn: Callable=English) -> list:\n",
    "    \"\"\"Convert text into list of tokens.\"\"\"\n",
    "    nlp = model_fn()\n",
    "    if special is not None:\n",
    "        for t in special:\n",
    "            nlp.tokenizer.add_special_case(t, [{spacy.symbols.ORTH: t}])\n",
    "    text = combine(text, *prep)\n",
    "    tokens = [token.text for token in nlp.make_doc(text)]\n",
    "    tokens = combine(tokens, *post)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def tokenize_english(text):\n",
    "    return tokenize(text, prep=PREP_RULES, post=POST_RULES, special=TOKENS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"English text that should be tokenized.\n",
    "\n",
    "The text contains \"quoted names\", commas, dots. It also has some shortcuts, like \"doesn't\"\n",
    "and \"don't\", if you'd like. \n",
    "\n",
    "Also, we've SOME CAPSLOCK here.\n",
    "\"\"\"\n",
    "\n",
    "expected = [\n",
    "    T_MAJ, 'english', 'text', 'that', 'should', 'be', 'tokenized', '.', '\\n \\n ',\n",
    "    T_MAJ, 'the', 'text', 'contains', '\"', 'quoted', 'names', '\"', ',', 'commas',\n",
    "    ',', 'dots', '.',\n",
    "    T_MAJ, 'it', 'also', 'has', 'some', 'shortcuts', ',', 'like', '\"', 'does',\n",
    "    \"n't\", '\"', '\\n ', 'and', '\"', 'do', \"n't\", '\"', ',', 'if', 'you', \"'d\", 'like',\n",
    "    '.', '\\n \\n ',\n",
    "    T_MAJ, 'also', ',', 'we', \"'ve\", T_UP, 'some', T_UP, 'capslock', 'here', '.', '\\n '\n",
    "]\n",
    "\n",
    "assert tokenize_english(text) == expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def useless_token(token, remove=('=', ' ')):\n",
    "    return token in remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def create_samples(tokens, eos=T_EOS, ignore=useless_token):\n",
    "    \"\"\"Splits list of tokens into samples using EOS tokens as delimiters.\"\"\"\n",
    "    samples, run = [], []\n",
    "    for token in tokens:\n",
    "        if ignore(token):\n",
    "            continue\n",
    "        run.append(token)\n",
    "        if token == eos:\n",
    "            samples.append(run)\n",
    "            run = []\n",
    "    if run:\n",
    "        samples.append(run)\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def format_tokens(tokens): return SEP.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def print_tokens(tokens, n=500): print(format_tokens(tokens[:n]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xxmajâ€¢englishâ€¢textâ€¢thatâ€¢shouldâ€¢beâ€¢tokenizedâ€¢.â€¢\n",
      " \n",
      " â€¢xxmajâ€¢theâ€¢textâ€¢containsâ€¢\"â€¢quotedâ€¢namesâ€¢\"â€¢,â€¢commasâ€¢,â€¢dotsâ€¢.â€¢xxmajâ€¢itâ€¢alsoâ€¢hasâ€¢someâ€¢shortcutsâ€¢,â€¢likeâ€¢\"â€¢doesâ€¢n'tâ€¢\"â€¢\n",
      " â€¢andâ€¢\"â€¢doâ€¢n'tâ€¢\"â€¢,â€¢ifâ€¢youâ€¢'dâ€¢likeâ€¢.â€¢\n",
      " \n",
      " â€¢xxmajâ€¢alsoâ€¢,â€¢weâ€¢'veâ€¢xxupâ€¢someâ€¢xxupâ€¢capslockâ€¢hereâ€¢.â€¢\n",
      " \n"
     ]
    }
   ],
   "source": [
    "print_tokens(expected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def read_files(root, labels=None, ext='txt', as_pandas=False):\n",
    "    \"\"\"Reads files from folders, using each one as a label name.\"\"\"\n",
    "    texts = []\n",
    "    for path in Path(root).iterdir():\n",
    "        if path.is_dir():\n",
    "            label = path.stem\n",
    "            if labels is not None and label in labels:\n",
    "                continue\n",
    "            items = [\n",
    "                {'text': fn.open().read(), 'name': fn.stem, 'label': label}\n",
    "                for fn in path.glob(f'*.{ext}')]\n",
    "            texts += items\n",
    "    return pd.DataFrame(texts) if as_pandas else texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb = read_files('/home/ck/data/imdb/train', as_pandas=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def parallel_tokenizer(texts, tokenizer_fn, chunk_size=10000, n_jobs=None,\n",
    "                       backend=None, as_pandas=False):\n",
    "    n_jobs = n_jobs or cpu_count()\n",
    "    with Parallel(n_jobs=n_jobs, backend=backend) as parallel:\n",
    "        results = parallel(delayed(tokenizer_fn)(ch) for ch in chunks(texts))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'chunks' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-82-cab6d0d9498f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparallel_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenize_english\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-80-0d2258e06752>\u001b[0m in \u001b[0;36mparallel_tokenizer\u001b[0;34m(texts, tokenizer_fn, chunk_size, n_jobs, backend, as_pandas)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mn_jobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mcpu_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mParallel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mparallel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparallel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelayed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mchunks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'chunks' is not defined"
     ]
    }
   ],
   "source": [
    "tokens = parallel_tokenizer(imdb.text, tokenize_english)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastai (cuda 10)",
   "language": "python",
   "name": "fastai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
