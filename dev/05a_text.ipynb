{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\"\"\"\n",
    "Text processing utils. Mostly copied from the fastai library.\n",
    "\n",
    "The utilities help to convert \"raw\" texts into formats more suitable for\n",
    "NLP models. The texts are cleaned and converted into list of tokens.\n",
    "\"\"\"\n",
    "from collections import Counter, OrderedDict\n",
    "from itertools import chain\n",
    "import html\n",
    "from multiprocessing import cpu_count\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "\n",
    "from loop.annotations import MaybeList, Callable\n",
    "from loop.utils import combine, chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "SEP    = '•'\n",
    "T_UNK  = 'xxunk'\n",
    "T_PAD  = 'xxpad'\n",
    "T_BOS  = 'xxbos'\n",
    "T_EOS  = 'xxeos'\n",
    "T_REP  = 'xxrep'\n",
    "T_WREP = 'xxwrep'\n",
    "T_UP   = 'xxup'\n",
    "T_MAJ  = 'xxmaj'\n",
    "TOKENS = [T_UNK, T_PAD, T_BOS, T_EOS, T_REP, T_WREP, T_UP, T_MAJ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def replace_tabs_with_spaces(s: str) -> str: return s.replace('\\t', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert replace_tabs_with_spaces('\\ttabs\\t') == ' tabs '\n",
    "assert replace_tabs_with_spaces('\\t\\t\\tmore tabs\\t\\t\\t') == '   more tabs   '\n",
    "assert replace_tabs_with_spaces('noop') == 'noop'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def add_spaces_around(s: str) -> str: return re.sub(r'([/#\\n])', r' \\1 ', s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert add_spaces_around('#') == ' # '\n",
    "assert add_spaces_around('\\n') == ' \\n '\n",
    "assert add_spaces_around('noop') == 'noop'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def trim_useless_spaces(s: str) -> str: return re.sub(' {2,}', ' ', s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "space = ' '\n",
    "assert all([trim_useless_spaces(space * i) == space for i in range (1, 11)])\n",
    "assert trim_useless_spaces(f'{space}word{space}') == f'{space}word{space}'\n",
    "assert trim_useless_spaces('noop') == 'noop'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def replace_repeated_chars(s: str) -> str:\n",
    "    def _replace(match):\n",
    "        char, repeats = match.groups()\n",
    "        return f' {T_REP} {len(repeats) + 1} {char} '\n",
    "    regex = re.compile(r'(\\S)(\\1{3,})')\n",
    "    return regex.sub(_replace, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert replace_repeated_chars('aaaa') == f' {T_REP} 4 a '\n",
    "assert replace_repeated_chars('sooooo cooool') == f's {T_REP} 5 o  c {T_REP} 4 o l'\n",
    "assert replace_repeated_chars('noop') == 'noop'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def replace_repeated_words(s: str) -> str:\n",
    "    def _replace(match):\n",
    "        word, repeats = match.groups()\n",
    "        return f' {T_WREP} {len(repeats.split()) + 1} {word} '\n",
    "    regex = re.compile(r'(\\b\\w+\\W+)(\\1{3,})')\n",
    "    return regex.sub(_replace, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert replace_repeated_words('one one one one one') == f' {T_WREP} 4 one  one'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def replace_br_tags(s: str) -> str: return re.sub(r'<[\\s]*br[\\s]*/[\\s]*>', '\\n', s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def fix_special_cases(s: str) -> str:\n",
    "    regex = re.compile(r'  +')\n",
    "    s = (s.\n",
    "         replace('#39;',  \"'\").replace('amp;',    '&').replace('#146;',   \"'\").\n",
    "         replace('nbsp;', ' ').replace('#36;',    '$').replace('\\\\n',    \"\\n\").\n",
    "         replace('quot;', \"'\").replace('\\\\\"',     '\"').replace(' @.@ ',   '.').\n",
    "         replace(' @-@ ', '-').replace(' @,@ ',   ',').replace('\\\\',   ' \\\\ ').\n",
    "         replace('<unk>', T_UNK))\n",
    "    return regex.sub(' ', html.unescape(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def replace_new_lines(s: str) -> str: return s.replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def replace_capslock(tokens: list) -> list:\n",
    "    new = []\n",
    "    for token in tokens:\n",
    "        if token.isupper() and len(token) > 1:\n",
    "            new += [T_UP, token.lower()]\n",
    "        else:\n",
    "            new.append(token)\n",
    "    return new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert replace_capslock(['CAPSLOCK']) == [T_UP, 'capslock']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def replace_capitalized(tokens: list) -> list:\n",
    "    new = []\n",
    "    for token in tokens:\n",
    "        if token == '':\n",
    "            continue\n",
    "        if token[0].isupper() and len(token) > 1 and token[1:].islower():\n",
    "            new.append(T_MAJ)\n",
    "        new.append(token.lower())\n",
    "    return new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert replace_capitalized(['Capitalized', 'Words']) == [T_MAJ, 'capitalized', T_MAJ, 'words']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "PREP_RULES = [\n",
    "    replace_tabs_with_spaces,\n",
    "    replace_br_tags,\n",
    "    fix_special_cases,\n",
    "    replace_repeated_chars,\n",
    "    replace_repeated_words,\n",
    "    replace_new_lines,\n",
    "    trim_useless_spaces\n",
    "]\n",
    "\n",
    "POST_RULES = [\n",
    "    replace_capslock,\n",
    "    replace_capitalized\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "def clean_text(s: str, rules=None):\n",
    "    rules = rules or PREP_RULES\n",
    "    return combine(s, *rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def update_tokens(tokens: str, rules=None):\n",
    "    rules = rules or POST_RULES\n",
    "    return combine(tokens, *rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def tokenize_english(texts: list):\n",
    "    return tokenize(texts, chunk_size=100_000, num_workers=cpu_count(), special=TOKENS)\n",
    "\n",
    "\n",
    "def tokenize(texts: list, chunk_size: int, num_workers: int=1, \n",
    "             model_fn=English, prep=clean_text, post=update_tokens,\n",
    "             special=None, backend='loky'):\n",
    "    \n",
    "    def doc_to_list(doc: str):\n",
    "        return [token.text for token in doc]\n",
    "    \n",
    "    def worker(nlp, texts):\n",
    "        return [post(doc_to_list(nlp.make_doc(prep(text)))) for text in texts]\n",
    "    \n",
    "    if len(texts) <= 2*chunk_size:\n",
    "        nlp = init_tokenizer(model_fn, special)\n",
    "        return worker(nlp, texts)\n",
    "    \n",
    "    with Parallel(n_jobs=num_workers, backend=backend) as parallel:\n",
    "        results = parallel(\n",
    "            delayed(worker)(nlp, text_chunk)\n",
    "            for nlp, text_chunk in (\n",
    "                (init_tokenizer(model_fn, special), t) \n",
    "                for t in chunks(texts, chunk_size)\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    return list(chain(*results))\n",
    "\n",
    "\n",
    "def init_tokenizer(model_fn, special=None):\n",
    "    nlp = model_fn()\n",
    "    if special is not None:\n",
    "        for t in special:\n",
    "            nlp.tokenizer.add_special_case(t, [{spacy.symbols.ORTH: t}])\n",
    "    return nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'xxmaj•english•text•that•should•be•tokenized•.•xxmaj•the•text•contains•\"•quoted•names•\"•,•commas•,•dots•.•xxmaj•it•also•has•some•shortcuts•,•like•\"•does•n\\'t•\"•and•\"•do•n\\'t•\"•,•if•you•\\'d•like•.•xxmaj•also•,•we•\\'ve•xxup•some•xxup•capslock•here•.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"\"\"English text that should be tokenized.\n",
    "\n",
    "The text contains \"quoted names\", commas, dots. It also has some shortcuts, like \"doesn't\"\n",
    "and \"don't\", if you'd like. \n",
    "\n",
    "Also, we've SOME CAPSLOCK here.\n",
    "\"\"\"\n",
    "\n",
    "'•'.join(tokenize_english([text])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def useless_token(token, remove=('=', ' ')):\n",
    "    return token in remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def format_tokens(tokens): return SEP.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def print_tokens(tokens): print(format_tokens(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def read_files(root, labels=None, ext='txt', as_pandas=False):\n",
    "    \"\"\"Reads files from folders, using each one as a label name.\"\"\"\n",
    "    texts = []\n",
    "    for path in Path(root).expanduser().iterdir():\n",
    "        if path.is_dir():\n",
    "            label = path.stem\n",
    "            if labels is not None and label in labels:\n",
    "                continue\n",
    "            items = [\n",
    "                {'text': fn.open().read(), 'name': fn.stem, 'label': label}\n",
    "                for fn in path.glob(f'*.{ext}')]\n",
    "            texts += items\n",
    "    return pd.DataFrame(texts) if as_pandas else texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb = read_files('~/data/imdb/train', as_pandas=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    def __init__(self, strings, size=60000, min_freq=2, special=TOKENS):\n",
    "        vocab = sorted([w for w, c in Counter(strings).most_common(size) if c >= min_freq])\n",
    "\n",
    "        if special is not None:\n",
    "            for tok in reversed(special):\n",
    "                if tok in vocab:\n",
    "                    vocab.remove(tok)\n",
    "                vocab.insert(0, tok)\n",
    "            \n",
    "        self.itos = OrderedDict(enumerate(vocab))\n",
    "        self.stoi = OrderedDict([(v, k) for k, v in self.itos.items()])\n",
    "        \n",
    "    @staticmethod\n",
    "    def from_token_lists(lists_of_tokens, **kwargs):\n",
    "        return Vocab(list(chain.from_iterable(lists_of_tokens)), **kwargs)\n",
    "    \n",
    "    @property\n",
    "    def words(self): return list(self.stoi.keys())\n",
    "\n",
    "    def __len__(self): return len(self.itos)\n",
    "    \n",
    "    def __iter__(self): \n",
    "        return iter(self.itos.items())\n",
    "    \n",
    "    def __call__(self, strings):\n",
    "        return [self.stoi[string] for string in strings]\n",
    "        \n",
    "    def __getitem__(self, value):\n",
    "        if isinstance(value, str):\n",
    "            return self.stoi.get(value, 0)\n",
    "        elif isinstance(value, int):\n",
    "            return self.itos.get(value, T_UNK)\n",
    "        raise TypeError(f'unexpected index type: {type(value)}, should be str or int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenize_english(imdb.text.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"...when he remade Broadway BILL (1934) as RIDING HIGH (1950). Recasting Bing Crosby as DAN BROOKS did not help a screenplay that was 'dated' in 34 let alone 50. This sad film has entire scenes lifted from the original with many of the supporting cast repeating their roles, unless they were dead. Though being older did not seem to matter to the Director. Nor that the Cars and Clothes in the background plates from 1934 did not seem match up too 1950s' standards. Not even 'der Bingel' singing can redeem this effort.<br /><br />We rated both the original and the remake IMDb Four****Stars. Frank's touch was long gone and all that was left was CAPRA-CORN. That did not stop Mr. Capra though. After floundering around the 50's making some educational documentaries he wound up his career remaking LADY FOR A DAY (1933) as POCKETFUL OF MIRACLES (1961). Again a fine cast was let down on that IMDb Six******Star effort compared too the originals Eight********Stars. Sometimes it is better to quit while you were still ahead, right after STATE OF THE UNION (1948).\""
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb.text[15142]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...•when•he•remade•xxmaj•broadway•xxup•bill•(•1934•)•as•xxup•riding•xxup•high•(•1950•)•.•xxmaj•recasting•xxmaj•bing•xxmaj•crosby•as•xxup•dan•xxup•brooks•did•not•help•a•screenplay•that•was•'•dated•'•in•34•let•alone•50•.•xxmaj•this•sad•film•has•entire•scenes•lifted•from•the•original•with•many•of•the•supporting•cast•repeating•their•roles•,•unless•they•were•dead•.•xxmaj•though•being•older•did•not•seem•to•matter•to•the•xxmaj•director•.•xxmaj•nor•that•the•xxmaj•cars•and•xxmaj•clothes•in•the•background•plates•from•1934•did•not•seem•match•up•too•1950s•'•standards•.•xxmaj•not•even•'•der•xxmaj•bingel•'•singing•can•redeem•this•effort•.•xxmaj•we•rated•both•the•original•and•the•remake•imdb•xxmaj•four•xxrep•4•*•xxmaj•stars•.•xxmaj•frank•'s•touch•was•long•gone•and•all•that•was•left•was•xxup•capra•-•xxup•corn•.•xxmaj•that•did•not•stop•xxmaj•mr.•xxmaj•capra•though•.•xxmaj•after•floundering•around•the•50•'s•making•some•educational•documentaries•he•wound•up•his•career•remaking•xxup•lady•xxup•for•a•xxup•day•(•1933•)•as•xxup•pocketful•xxup•of•xxup•miracles•(•1961•)•.•xxmaj•again•a•fine•cast•was•let•down•on•that•imdb•xxmaj•six•xxrep•6•*•xxmaj•star•effort•compared•too•the•originals•xxmaj•eight•xxrep•8•*•xxmaj•stars•.•xxmaj•sometimes•it•is•better•to•quit•while•you•were•still•ahead•,•right•after•xxup•state•xxup•of•xxup•the•xxup•union•(•1948•)•.\n"
     ]
    }
   ],
   "source": [
    "print_tokens(tokens[15142])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Vocab.from_token_lists(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['xxunk',\n",
       " 'xxpad',\n",
       " 'xxbos',\n",
       " 'xxeos',\n",
       " 'xxrep',\n",
       " 'xxwrep',\n",
       " 'xxup',\n",
       " 'xxmaj',\n",
       " ' ',\n",
       " '!',\n",
       " '\"',\n",
       " '\")',\n",
       " '\"a',\n",
       " '\"beirut',\n",
       " '\"it',\n",
       " '\"the',\n",
       " '\"what',\n",
       " '#',\n",
       " '$',\n",
       " '%']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.words[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastai (cuda 10)",
   "language": "python",
   "name": "fastai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
