# -----------------------------------------
# THIS FILE WAS AUTOGENERATED! DO NOT EDIT!
# -----------------------------------------
# file to edit: 05a_text.ipynb

"""
Text processing utils. Mostly copied from the fastai library.

The utilities help to convert "raw" texts into formats more suitable for
NLP models. The texts are cleaned and converted into list of tokens.
"""
import html
import re

import spacy
from spacy.lang.en import English

from loop.annotations import MaybeList, Callable
from loop.utils import combine


SEP    = 'â€¢'
T_UP   = 'xxup'
T_REP  = 'xxrep'
T_WREP = 'xxwrep'
T_MAJ  = 'xxmaj'
T_BOS  = 'xxbos'
T_EOS  = 'xxeos'
T_FLD  = 'xxfld'
T_UNK  = 'xxunk'
T_PAD  = 'xxpad'
TOKENS = [T_UP, T_REP, T_WREP, T_MAJ, T_BOS, T_EOS, T_FLD, T_UNK, T_PAD]


def replace_tabs_with_spaces(s: str) -> str: return s.replace('\t', ' ')


def add_spaces_around(s: str) -> str: return re.sub(r'([/#\n])', r' \1 ', s)


def trim_useless_spaces(s: str) -> str: return re.sub(' {2,}', ' ', s)


def replace_repeated_chars(s: str) -> str:
    def _replace(match):
        char, repeats = match.groups()
        return f' {T_REP} {len(repeats) + 1} {char} '
    regex = re.compile(r'(\S)(\1{3,})')
    return regex.sub(_replace, s)


def replace_repeated_words(s: str) -> str:
    def _replace(match):
        word, repeats = match.groups()
        return f' {T_WREP} {len(repeats.split()) + 1} {word} '
    regex = re.compile(r'(\b\w+\W+)(\1{3,})')
    return regex.sub(_replace, s)


def fix_wikitext_special_cases(s: str) -> str:
    regex = re.compile(r'  +')
    s = (s.
         replace('#39;',    "'").replace('amp;',    '&').replace('#146;', "'").
         replace('nbsp;',   ' ').replace('#36;',    '$').replace('\\n',  "\n").
         replace('quot;',   "'").replace('<br />', "\n").replace('\\"',   '"').
         replace(' @.@ ',   '.').replace(' @-@ ',   '-').replace(' @,@ ', ',').
         replace('\\',   ' \\ ').replace('<unk>', T_UNK))
    return regex.sub(' ', html.unescape(s))


def replace_capslock(tokens: list) -> list:
    new = []
    for token in tokens:
        if token.isupper() and len(token) > 1:
            new += [T_UP, token.lower()]
        else:
            new.append(token)
    return new


def replace_capitalized(tokens: list) -> list:
    new = []
    for token in tokens:
        if token == '':
            continue
        if token[0].isupper() and len(token) > 1 and token[1:].islower():
            new.append(T_MAJ)
        new.append(token.lower())
    return new


PREP_RULES = [
    replace_tabs_with_spaces,
    add_spaces_around,
    trim_useless_spaces,
    replace_repeated_chars,
    replace_repeated_words,
    fix_wikitext_special_cases,
]

POST_RULES = [
    replace_capslock,
    replace_capitalized
]


def tokenize(text: str, prep: MaybeList=None, post: MaybeList=None,
             special: MaybeList=None, model_fn: Callable=English) -> list:
    """Convert text into list of tokens."""
    nlp = model_fn()
    if special is not None:
        for t in special:
            nlp.tokenizer.add_special_case(t, [{spacy.symbols.ORTH: t}])
    text = combine(text, *prep)
    tokens = [token.text for token in nlp.make_doc(text)]
    tokens = combine(tokens, *post)
    return tokens


def tokenize_english(text):
    return tokenize(text, prep=PREP_RULES, post=POST_RULES, special=TOKENS)


def useless_token(token, remove=('=', ' ')):
    return token in remove


def create_samples(tokens, eos=T_EOS, ignore=useless_token):
    """Splits list of tokens into samples using EOS tokens as delimiters."""
    samples, run = [], []
    for token in tokens:
        if ignore(token):
            continue
        run.append(token)
        if token == eos:
            samples.append(run)
            run = []
    if run:
        samples.append(run)
    return samples


def format_tokens(tokens): return SEP.join(tokens)


def print_tokens(tokens, n=500): print(format_tokens(tokens[:n]))
