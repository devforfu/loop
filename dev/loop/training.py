# -----------------------------------------
# THIS FILE WAS AUTOGENERATED! DO NOT EDIT!
# -----------------------------------------
# file to edit: 01_training.ipynb

from collections import OrderedDict
from typing import Union, Tuple
from enum import IntEnum
from operator import itemgetter

import numpy as np
import torch
from torch.utils.data import DataLoader, TensorDataset

from loop.config import defaults
from loop.utils import merge_dicts, to_list


class Phase:
    """
    Model training loop phase.

    Each model's training loop iteration could be separated into (at least) two
    phases: training and validation. The instances of this class track
    metrics and counters, related to the specific phase, and keep the reference
    to subset of data, used during phase.
    """
    def __init__(self, name: str, loader: 'DataLoader', grad: bool=True):
        self.name = name
        self.loader = loader
        self.grad = grad
        self.batch_loss = None
        self.batch_index = 0
        self.rolling_loss = 0
        self.losses = []
        self.metrics = OrderedDict()

    @property
    def last_loss(self):
        return self.losses[-1] if self.losses else None

    @property
    def last_metrics(self):
        metrics = OrderedDict()
        metrics[f'{self.name}_loss'] = self.last_loss
        for name, values in self.metrics.items():
            metrics[f'{self.name}_{name}'] = values[-1]
        return metrics

    @property
    def metrics_history(self):
        metrics = OrderedDict()
        for name, values in self.metrics.items():
            metrics[f'{self.name}_{name}'] = values
        return metrics

    def update(self, loss):
        self.losses.append(loss)

    def update_metric(self, name, value):
        if name not in self.metrics:
            self.metrics[name] = []
        self.metrics[name].append(value)

    @staticmethod
    def make_train_valid(trn_ds, val_ds,
                         bs: int=defaults.bs,
                         num_workers: Union[Tuple, int]=0):
        """Creates two loop's phases, train and valid.

        The phases are thin wrappers on top of data loaders intended to track
        additional information gathered during model's fitting process, like
        loss, performance metrics, etc.
        """
        trn, val = unwrap(num_workers, 2)
        phs = OrderedDict()
        phs['train'] = Phase('train', DataLoader(trn_ds, bs, shuffle=True, num_workers=trn))
        phs['valid'] = Phase('valid', DataLoader(val_ds, bs, num_workers=val), grad=False)
        return phs


def is_scalar(obj):
    return isinstance(obj, (int, float, str, complex))


def unwrap(obj, pad=1):
    """Convenience function to unwrap collections and broadcast scalars."""
    if is_scalar(obj):
        return [obj]*pad
    return obj


class Callback:
    """The base class inherited by callbacks.

    Provides a lot of hooks invoked on various stages of the training loop
    execution. The signature of functions is as broad as possible to allow
    flexibility and customization in descendant classes.
    """
    def training_started(self, **kwargs): pass
    def training_ended(self, **kwargs): pass
    def epoch_started(self, **kwargs): pass
    def epoch_ended(self, **kwargs): pass
    def phase_started(self, **kwargs): pass
    def phase_ended(self, **kwargs): pass
    def batch_started(self, **kwargs): pass
    def batch_ended(self, **kwargs): pass
    def before_forward(self, **kwargs): pass
    def after_forward(self, **kwargs): pass
    def before_backward(self, **kwargs): pass
    def after_backward(self, **kwargs): pass
    def interrupted(self, **kwargs): pass


class RollingLoss(Callback):
    """A callback that tracks model's loss.

    The loss is interpolated between current and next value to get
    a smoother loss curve.
    """
    order = Order.Loss()

    def __init__(self, smooth=0.98):
        self.smooth = smooth

    def batch_ended(self, phase, **kwargs):
        prev = phase.rolling_loss
        a = self.smooth
        avg_loss = a*prev + (1 - a)*phase.batch_loss
        debias_loss = avg_loss / (1 - a**phase.batch_index)
        phase.rolling_loss = avg_loss
        phase.update(debias_loss)

    def epoch_ended(self, phases, **kwargs):
        for phase in phases:
            phase.update_metric('loss', phase.last_loss)


class History(Callback):
    """A callback that collects model's metrics during its training."""

    order = Order.History()

    def training_started(self, **kwargs):
        self.recorded = None
        self.epochs = []

    def epoch_ended(self, epoch, **kwargs):
        self.epochs.append(epoch)

    def training_ended(self, phases, **kwargs):
        epochs = {'epoch': np.array(self.epochs).astype(int)}
        metrics = [epochs] + [p.metrics_history for p in phases]
        data = pd.DataFrame(merge_dicts(metrics))
        data.reset_index(inplace=True, drop=True)
        self.recorded = data

    def plot(self, x='epoch', ys='loss', ax=None):
        return self.recorded.plot(x='epoch', y=to_list(ys), ax=ax)


class Group(Callback):
    def __init__(self, cbs):
        self._init(self, cbs)

    def _init(self, cbs):
        cbs = Order.sort(cbs)
        self.callbacks = cbs
        self.named_callbacks = {to_snake_case(classname(cb)): cb for cb in cbs}

    def add(self, cb, *cbs):
        cbs = [cb] + list(cbs)
        self._init(cbs)

    def training_started(self, **kwargs): self('training_started')
    def training_ended(self, **kwargs): self('training_ended')
    def epoch_started(self, **kwargs): self('epoch_started')
    def epoch_ended(self, **kwargs): self('epoch_ended')
    def phase_started(self, **kwargs): self('phase_started')
    def phase_ended(self, **kwargs): self('phase_ended')
    def batch_started(self, **kwargs): self('batch_started')
    def batch_ended(self, **kwargs): self

    def before_forward(self, **kwargs): pass

    def after_forward(self, **kwargs): pass

    def before_backward(self, **kwargs): pass

    def after_backward(self, **kwargs): pass

    def interrupted(self, **kwargs): pass


    def __getitem__(self, item):
        item = to_snake_case(item)
        if item in self.named_callbacks:
            return self.named_callbacks[item]
        raise KeyError(
            f'callback name is not found: {item}; '
            f'available callbacks are: {list(sorted(self.named_callbacks))}')
