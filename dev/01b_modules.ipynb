{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.modules import activation as torch_act\n",
    "\n",
    "from loop.annotations import MaybeActivation, ListOfModules\n",
    "from loop.utils import pairs\n",
    "\n",
    "\n",
    "__all__ = ['get_activation', 'fc', 'conv2d', 'bottleneck', 'Flatten',\n",
    "           'AdaptiveConcatPool2d', 'TinyNet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "_public_names = [\n",
    "    (name, cls) for name, cls in \n",
    "    ((name, getattr(torch_act, name)) for name in dir(torch_act) \n",
    "     if not name.startswith('_')) \n",
    "    if type(cls) == type and issubclass(cls, nn.Module) and cls is not nn.Module]\n",
    "\n",
    "_name_to_cls = {name.lower(): cls for name, cls in _public_names}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_activation(name: str) -> MaybeActivation:\n",
    "    \"\"\"Convenience function that creates activation function from string.\n",
    "\n",
    "    A string can include not only an activation function name but also positional and/or \n",
    "    keyword parameters accepted by function initialization. Therefore, the string could \n",
    "    have one of the following formats:\n",
    "        * name\n",
    "        * name:value1;value2;...\n",
    "        * name:value1;param2=value2;...\n",
    "        * name:param1=value1;param2=value2;...\n",
    "\n",
    "    If not a string was provided but instance of torch.nn.Module, then the object is returned\n",
    "    as is.\n",
    "\n",
    "    Examples:\n",
    "        get_activation('relu')\n",
    "        get_activation('relu:inplace=True')\n",
    "        get_activation('prelu:3')\n",
    "\n",
    "    \"\"\"\n",
    "    if isinstance(name, nn.Module):\n",
    "        return name\n",
    "\n",
    "    if name is None or name.lower() in ('linear', 'none'):\n",
    "        return None\n",
    "\n",
    "    args, kwargs = [], {}\n",
    "    if ':' in name:\n",
    "        name, params = name.split(':')\n",
    "        for param in params.split(';'):\n",
    "            if '=' in param:\n",
    "                key, value = param.split('=')\n",
    "                kwargs[key] = value\n",
    "            else:\n",
    "                args.append(param)\n",
    "    cls = _name_to_cls[name.lower()]\n",
    "    return cls(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "act = get_activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert isinstance(act('relu'), torch_act.ReLU)\n",
    "assert isinstance(act('ReLU'), torch_act.ReLU)\n",
    "assert isinstance(act('RELU'), torch_act.ReLU)\n",
    "assert isinstance(act('relu:inplace=True'), torch_act.ReLU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class TinyNet(nn.Module):\n",
    "    \"\"\"Simplistic convolution network.\n",
    "\n",
    "    Something suitable for tests and simple datasets training but probably nothing more.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_channels=3, n_out=10, activation=None):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(n_channels, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(500, 50)\n",
    "        self.fc2 = nn.Linear(50, n_out)\n",
    "        self.activation = activation\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        if self.activation is not None:\n",
    "            x = self.activation(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class AdaptiveConcatPool2d(nn.Module):\n",
    "    \"\"\"Applies average and maximal adaptive pooling to the tensor and\n",
    "    concatenates results into a single tensor.\n",
    "\n",
    "    The idea is taken from fastai library.\n",
    "    \"\"\"\n",
    "    def __init__(self, size=1):\n",
    "        super().__init__()\n",
    "        self.avg = nn.AdaptiveAvgPool2d(size)\n",
    "        self.max = nn.AdaptiveMaxPool2d(size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.cat([self.max(x), self.avg(x)], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Flatten(nn.Module):\n",
    "    \"\"\"Converts N-dimensional tensor into 'flat' one.\"\"\"\n",
    "\n",
    "    def __init__(self, keep_batch_dim=True):\n",
    "        super().__init__()\n",
    "        self.keep_batch_dim = keep_batch_dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.keep_batch_dim:\n",
    "            return x.view(x.size(0), -1)\n",
    "        return x.view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class SeparableConv(nn.Module):\n",
    "    \"\"\"Simple implementation of N-d separable convolution.\"\"\"\n",
    "    \n",
    "    def __init__(self, ni, no, kernel=3, stride=1, pad=0, conv=nn.Conv1d):\n",
    "        super().__init__()\n",
    "        self.depthwise = conv(ni, ni, kernel, stride, padding=pad, groups=ni)\n",
    "        self.pointwise = conv(ni, no, kernel_size=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.pointwise(self.depthwise(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def fc(ni: int, no: int, bias: bool=True, bn: bool=True, activ: str='linear',\n",
    "       dropout: float=None) -> ListOfModules:\n",
    "    \"\"\"Convenience function that creates a linear layer instance with the 'batteries included'.\n",
    "\n",
    "    The list of created layers:\n",
    "        * Linear\n",
    "        * (optionally) BatchNorm\n",
    "        * (optionally) Dropout\n",
    "        * (optionally) Activation function\n",
    "\n",
    "    \"\"\"\n",
    "    layers = [nn.Linear(ni, no, bias)]\n",
    "    if bn:\n",
    "        layers.append(nn.BatchNorm1d(no))\n",
    "    if activ is not None:\n",
    "        func = act(activ)\n",
    "        if func is not None:\n",
    "            layers.append(func)\n",
    "    if dropout is not None:\n",
    "        layers.append(nn.Dropout(dropout))\n",
    "    return layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def conv2d(ni: int, no: int, kernel: int, stride: int, groups: int=1, \n",
    "           lrn: bool=False, bn: bool=False, pad: int=0, pool: tuple=None,\n",
    "           activ: str='prelu') -> ListOfModules:\n",
    "    \"\"\"Convenience function that creates a 2D conv layer with the 'batteries included'.\n",
    "\n",
    "    The list of created layers:\n",
    "        * Convolution layer\n",
    "        * (optionally) Activation\n",
    "        * (optionally) Local response norm OR Batch norm\n",
    "        * (optionally) Pooling\n",
    "\n",
    "    \"\"\"\n",
    "    bias = not (lrn or bn)\n",
    "    layers = [nn.Conv2d(ni, no, kernel, stride, pad, bias=bias, groups=groups)]\n",
    "    func = act(activ)\n",
    "    if func is not None:\n",
    "        layers += [func]\n",
    "    elif lrn:\n",
    "        layers.append(nn.LocalResponseNorm(2))\n",
    "    if pool is not None:\n",
    "        layers.append(nn.MaxPool2d(*pool))\n",
    "    return layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def sepconv(ni: int, no: int, kernel: int, stride: int, \n",
    "            pad: int, drop: float=None, activ: str='relu',\n",
    "            conv: nn.Module=nn.Conv1d) -> ListOfModules:\n",
    "    \"\"\"Convenience function that creates conv layer with the 'batteries included'.\"\"\"\n",
    "    \n",
    "    assert drop is None or (0.0 < drop < 1.0)\n",
    "    layers = [_SeparableConv(ni, no, kernel, stride, pad, conv=conv)]\n",
    "    layers.append(act(activ))\n",
    "    if drop is not None:\n",
    "        layers.append(nn.Dropout(drop))\n",
    "    return layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def bottleneck() -> ListOfModules:\n",
    "    \"\"\"A 'bridge' from convolutional blocks to fully-connected layers.\"\"\"\n",
    "    return [AdaptiveConcatPool2d(1), Flatten()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def fc_network(input_size: int, layers: list, activ: str='relu') -> nn.Sequential:\n",
    "    \"\"\"Creates simple fully-connected network.\n",
    "    \n",
    "    The `layers` list defines sizes of hidden layers and the output layers. Between \n",
    "    hidden layers activations are inserted. The topmost layer doesn't include any\n",
    "    non-linear activation.\n",
    "    \"\"\"\n",
    "    in_sz = input_size\n",
    "    model = []\n",
    "    for out_sz in layers[:-1]:\n",
    "        model += [nn.Linear(in_sz, out_sz), act(activ)]\n",
    "        in_sz = out_sz\n",
    "    model.append(nn.Linear(in_sz, layers[-1]))\n",
    "    return nn.Sequential(*model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_network(conv: list, bn: bool=True, kernel: int=3, activ: str='relu') -> nn.Sequential:\n",
    "    \"\"\"Creates simple 2d-convolutional network with adaptive pooling.\n",
    "    \n",
    "    The `conv` list defines sized of hidden layers and the fully-connected output layer.\n",
    "    \"\"\"\n",
    "    layers = []\n",
    "    for ni, no in pairs(conv[:-1]):\n",
    "        layers += [nn.Conv2d(ni, no, kernel, bias=not bn), act(activ)]\n",
    "        if bn:\n",
    "            layers.append(nn.BatchNorm2d(no))\n",
    "    ni, no = conv[-2:]\n",
    "    layers += bottleneck()\n",
    "    layers.append(nn.Linear(ni, no))\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "  (1): ReLU()\n",
       "  (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "  (4): ReLU()\n",
       "  (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "  (7): ReLU()\n",
       "  (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (9): AdaptiveConcatPool2d(\n",
       "    (avg): AdaptiveAvgPool2d(output_size=1)\n",
       "    (max): AdaptiveMaxPool2d(output_size=1)\n",
       "  )\n",
       "  (10): Flatten()\n",
       "  (11): Linear(in_features=128, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_network([3, 32, 64, 128, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastai (cuda 10)",
   "language": "python",
   "name": "fastai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
