{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.modules import activation as torch_act\n",
    "\n",
    "from loop.annotations import MaybeActivation, List, Func\n",
    "from loop.utils import pairs\n",
    "\n",
    "\n",
    "__all__ = ['get_activation', 'fc', 'conv2d', 'bottleneck', 'Flatten',\n",
    "           'AdaptiveConcatPool2d', 'TinyNet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "_public_names = [\n",
    "    (name, cls) for name, cls in \n",
    "    ((name, getattr(torch_act, name)) for name in dir(torch_act) \n",
    "     if not name.startswith('_')) \n",
    "    if type(cls) == type and issubclass(cls, nn.Module) and cls is not nn.Module]\n",
    "\n",
    "_name_to_cls = {name.lower(): cls for name, cls in _public_names}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_activation(name: str) -> MaybeActivation:\n",
    "    \"\"\"Convenience function that creates activation function from string.\n",
    "\n",
    "    A string can include not only an activation function name but also positional and/or \n",
    "    keyword parameters accepted by function initialization. Therefore, the string could \n",
    "    have one of the following formats:\n",
    "        * name\n",
    "        * name:value1;value2;...\n",
    "        * name:value1;param2=value2;...\n",
    "        * name:param1=value1;param2=value2;...\n",
    "\n",
    "    If not a string was provided but instance of torch.nn.Module, then the object is returned\n",
    "    as is.\n",
    "\n",
    "    Examples:\n",
    "        get_activation('relu')\n",
    "        get_activation('relu:inplace=True')\n",
    "        get_activation('prelu:3')\n",
    "\n",
    "    \"\"\"\n",
    "    if isinstance(name, nn.Module):\n",
    "        return name\n",
    "\n",
    "    if name is None or name.lower() in ('linear', 'none'):\n",
    "        return None\n",
    "\n",
    "    args, kwargs = [], {}\n",
    "    if ':' in name:\n",
    "        name, params = name.split(':')\n",
    "        for param in params.split(';'):\n",
    "            if '=' in param:\n",
    "                key, value = param.split('=')\n",
    "                kwargs[key] = value\n",
    "            else:\n",
    "                args.append(param)\n",
    "    cls = _name_to_cls[name.lower()]\n",
    "    return cls(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "act = get_activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert isinstance(act('relu'), torch_act.ReLU)\n",
    "assert isinstance(act('ReLU'), torch_act.ReLU)\n",
    "assert isinstance(act('RELU'), torch_act.ReLU)\n",
    "assert isinstance(act('relu:inplace=True'), torch_act.ReLU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class TinyNet(nn.Module):\n",
    "    \"\"\"Simplistic convolution network.\n",
    "\n",
    "    Something suitable for tests and simple datasets training but probably nothing more.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_channels=3, n_out=10, activation=None):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(n_channels, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(500, 50)\n",
    "        self.fc2 = nn.Linear(50, n_out)\n",
    "        self.activation = activation\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        if self.activation is not None:\n",
    "            x = self.activation(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class AdaptiveConcatPool2d(nn.Module):\n",
    "    \"\"\"Applies average and maximal adaptive pooling to the tensor and\n",
    "    concatenates results into a single tensor.\n",
    "\n",
    "    The idea is taken from fastai library.\n",
    "    \"\"\"\n",
    "    def __init__(self, size=1):\n",
    "        super().__init__()\n",
    "        self.avg = nn.AdaptiveAvgPool2d(size)\n",
    "        self.max = nn.AdaptiveMaxPool2d(size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.cat([self.max(x), self.avg(x)], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "pool = AdaptiveConcatPool2d()\n",
    "t = torch.ones((10, 3, 128, 128))\n",
    "out = pool(t)\n",
    "assert t.shape[0] == out.shape[0]\n",
    "assert t.shape[1]*2 == out.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Flatten(nn.Module):\n",
    "    \"\"\"Converts N-dimensional tensor into 'flat' one.\"\"\"\n",
    "\n",
    "    def __init__(self, keep_batch_dim=True):\n",
    "        super().__init__()\n",
    "        self.keep_batch_dim = keep_batch_dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.keep_batch_dim:\n",
    "            return x.view(x.size(0), -1)\n",
    "        return x.view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class SeparableConv(nn.Module):\n",
    "    \"\"\"Simple implementation of N-d separable convolution.\"\"\"\n",
    "    \n",
    "    def __init__(self, ni, no, kernel=3, stride=1, pad=0, conv=nn.Conv1d):\n",
    "        super().__init__()\n",
    "        self.depthwise = conv(ni, ni, kernel, stride, padding=pad, groups=ni)\n",
    "        self.pointwise = conv(ni, no, kernel_size=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.pointwise(self.depthwise(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def fc(ni: int, no: int, bias: bool=True, bn: bool=True, activ: str='linear',\n",
    "       dropout: float=None) -> List[nn.Module]:\n",
    "    \"\"\"Convenience function that creates a linear layer instance with the 'batteries included'.\n",
    "\n",
    "    The list of created layers:\n",
    "        * Linear\n",
    "        * (optionally) BatchNorm\n",
    "        * (optionally) Dropout\n",
    "        * (optionally) Activation function\n",
    "\n",
    "    \"\"\"\n",
    "    layers = [nn.Linear(ni, no, bias)]\n",
    "    if bn:\n",
    "        layers.append(nn.BatchNorm1d(no))\n",
    "    if activ is not None:\n",
    "        func = act(activ)\n",
    "        if func is not None:\n",
    "            layers.append(func)\n",
    "    if dropout is not None:\n",
    "        layers.append(nn.Dropout(dropout))\n",
    "    return layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def conv2d(ni: int, no: int, kernel: int, stride: int, groups: int=1, \n",
    "           lrn: bool=False, bn: bool=False, pad: int=0, pool: tuple=None,\n",
    "           activ: Func='prelu') -> List[nn.Module]:\n",
    "    \"\"\"Convenience function that creates a 2D conv layer with the 'batteries included'.\n",
    "\n",
    "    The list of created layers:\n",
    "        * Convolution layer\n",
    "        * (optionally) Activation\n",
    "        * (optionally) Local response norm OR Batch norm\n",
    "        * (optionally) Pooling\n",
    "\n",
    "    \"\"\"\n",
    "    bias = not (lrn or bn)\n",
    "    layers = [nn.Conv2d(ni, no, kernel, stride, pad, bias=bias, groups=groups)]\n",
    "    func = act(activ)\n",
    "    if func is not None:\n",
    "        layers += [func]\n",
    "    elif lrn:\n",
    "        layers.append(nn.LocalResponseNorm(2))\n",
    "    if pool is not None:\n",
    "        layers.append(nn.MaxPool2d(*pool))\n",
    "    return layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def sepconv(ni: int, no: int, kernel: int, stride: int, \n",
    "            pad: int, drop: float=None, activ: Func='relu',\n",
    "            conv: nn.Module=nn.Conv1d) -> List[nn.Module]:\n",
    "    \"\"\"Convenience function that creates conv layer with the 'batteries included'.\"\"\"\n",
    "    \n",
    "    assert drop is None or (0.0 < drop < 1.0)\n",
    "    layers = [_SeparableConv(ni, no, kernel, stride, pad, conv=conv)]\n",
    "    layers.append(act(activ))\n",
    "    if drop is not None:\n",
    "        layers.append(nn.Dropout(drop))\n",
    "    return layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def bottleneck() -> List[nn.Module]:\n",
    "    \"\"\"A 'bridge' from convolutional blocks to fully-connected layers.\"\"\"\n",
    "    return [AdaptiveConcatPool2d(1), Flatten()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def fc_network(input_size: int, layers: list, activ: Func='relu') -> nn.Sequential:\n",
    "    \"\"\"Creates simple fully-connected network.\n",
    "    \n",
    "    The `layers` list defines sizes of hidden layers and the output layers. Between \n",
    "    hidden layers activations are inserted. The topmost layer doesn't include any\n",
    "    non-linear activation.\n",
    "    \"\"\"\n",
    "    in_sz = input_size\n",
    "    model = []\n",
    "    for out_sz in layers[:-1]:\n",
    "        model += [nn.Linear(in_sz, out_sz), act(activ)]\n",
    "        in_sz = out_sz\n",
    "    model.append(nn.Linear(in_sz, layers[-1]))\n",
    "    return nn.Sequential(*model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def conv_network(conv: list, bn: bool=True, kernel: int=3, activ: Func='relu') -> nn.Sequential:\n",
    "    \"\"\"Creates simple 2d-convolutional network with adaptive pooling.\n",
    "    \n",
    "    The `conv` list defines sized of hidden layers and the fully-connected output layer.\n",
    "    \"\"\"\n",
    "    layers = []\n",
    "    for ni, no in pairs(conv[:-1]):\n",
    "        layers += [nn.Conv2d(ni, no, kernel, bias=not bn), act(activ)]\n",
    "        if bn:\n",
    "            layers.append(nn.BatchNorm2d(no))\n",
    "    ni, no = conv[-2:]\n",
    "    layers += bottleneck()\n",
    "    layers.append(nn.Linear(ni, no))\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def freeze_all(model: nn.Module):\n",
    "    \"\"\"Makes all model weights un-trainable.\"\"\"\n",
    "    for name, child in model.named_children():\n",
    "        for param in child.parameters():\n",
    "            param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "model = nn.Sequential(OrderedDict([\n",
    "    ('conv1', nn.Conv2d(3, 10, 3)),\n",
    "    ('conv2', nn.Conv2d(10, 32, 3)),\n",
    "    ('flat', Flatten()),\n",
    "    ('top', nn.Sequential(OrderedDict([\n",
    "        ('fc1', nn.Linear(1024, 512)),\n",
    "        ('fc2', nn.Linear(512, 1))\n",
    "    ])))\n",
    "]))\n",
    "\n",
    "freeze_all(model)\n",
    "\n",
    "for name, child in model.named_children():\n",
    "    for param in child.parameters():\n",
    "        assert not param.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def unfreeze_all(model):\n",
    "    \"\"\"Makes all model weights trainable.\"\"\"\n",
    "    for name, child in model.named_children():\n",
    "        for param in child.parameters():\n",
    "            param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unfreeze_all(model)\n",
    "\n",
    "for name, child in model.named_children():\n",
    "    for param in child.parameters():\n",
    "        assert param.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def unfreeze_layers(model, names):\n",
    "    \"\"\"Makes trainable only specific model \"\"\"\n",
    "    for name, child in model.named_children():\n",
    "        if name not in names:\n",
    "            continue\n",
    "        for param in child.parameters():\n",
    "            param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freeze_all(model)\n",
    "unfreeze_layers(model, ['top'])\n",
    "for name, child in model.named_children():\n",
    "    for param in child.parameters():\n",
    "        assert name != 'top' or (name == 'top' and param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_layer(model, key):\n",
    "    parts = key.split('.')\n",
    "    if len(parts) == 1:\n",
    "        return getattr(model, key)\n",
    "    else:\n",
    "        try:\n",
    "            for part in parts:\n",
    "                model = getattr(model, part)\n",
    "            return model\n",
    "        except AttributeError:\n",
    "            raise KeyError(f'cannot resolve key: {key}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def set_trainable(model, keys, trainable=True):\n",
    "    for key in keys:\n",
    "        layer = get_layer(model, key)\n",
    "        if trainable:\n",
    "            unfreeze_all(layer)\n",
    "        else:\n",
    "            freeze_all(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b1 = nn.Sequential(OrderedDict([\n",
    "    ('conv1', nn.Conv2d(3, 10, 3)),\n",
    "    ('relu1', nn.ReLU()),\n",
    "    ('conv2', nn.Conv2d(10, 32, 3)),\n",
    "    ('relu2', nn.ReLU())\n",
    "]))\n",
    "b2 = nn.Sequential(OrderedDict([\n",
    "    ('conv1', nn.Conv2d(32, 32, 3)),\n",
    "    ('relu1', nn.ReLU()),\n",
    "    ('conv2', nn.Conv2d(32, 10, 3))\n",
    "]))\n",
    "model = nn.Sequential(OrderedDict([\n",
    "    ('features', nn.Sequential(OrderedDict([\n",
    "        ('block1', b1),\n",
    "        ('block2', b2)\n",
    "    ])))\n",
    "]))\n",
    "\n",
    "assert get_layer(model, 'features.block1.conv1') is b1.conv1\n",
    "assert get_layer(model, 'features.block2.conv2') is b2.conv2\n",
    "assert get_layer(model, 'features.block1') is b1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_trainable(model, ['features'], trainable=False)\n",
    "for child in model.children():\n",
    "    for param in child.parameters():\n",
    "        assert not param.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_trainable(model, ['features.block1'])\n",
    "for child in b1.children():\n",
    "    for param in child.parameters():\n",
    "        assert param.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastai (cuda 10)",
   "language": "python",
   "name": "fastai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
