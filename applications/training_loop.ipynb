{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Training Loop Example\n",
    "\n",
    "The notebook contains an example of simple training loop initialization and training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torchvision import models\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import Compose, ToTensor, Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ck/anaconda3/envs/fastai/lib/python3.7/site-packages/tqdm/autonotebook/__init__.py:14: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  \" (e.g. in jupyter console)\", TqdmExperimentalWarning)\n"
     ]
    }
   ],
   "source": [
    "from tqdm.autonotebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "try:\n",
    "    old_path\n",
    "except NameError:\n",
    "    old_path = sys.path.copy()\n",
    "    sys.path = [Path.cwd().parent.as_posix()] + old_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import ChainMap, OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from loop.callbacks import History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flat_model(model):\n",
    "    \"\"\"Converts model with nested modules into single list of modules\"\"\"\n",
    "    \n",
    "    def flatten(m):\n",
    "        children = list(m.children())\n",
    "        if not children:\n",
    "            return [m]\n",
    "        return sum([flatten(child) for child in children], [])\n",
    "    \n",
    "    return nn.Sequential(*flatten(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def as_sequential(model):\n",
    "    return nn.Sequential(*list(model.children()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_output_shape(model):\n",
    "    \"\"\"Pass a dummy input through the sequential model to get the output tensor shape.\"\"\"\n",
    "    first, *rest = flat_model(model)\n",
    "    shape = first.in_channels, 128, 128\n",
    "    dummy_input = torch.zeros(shape)\n",
    "    out = model(dummy_input[None])\n",
    "    return list(out.size())[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaptiveConcatPool2d(nn.Module):\n",
    "    \n",
    "    def __init__(self, size=1):\n",
    "        super().__init__()\n",
    "        self.avg = nn.AdaptiveAvgPool2d(size)\n",
    "        self.max = nn.AdaptiveMaxPool2d(size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return torch.cat([self.max(x), self.avg(x)], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x.view(x.size(0), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_default(module, init_fn=nn.init.kaiming_normal_):\n",
    "    if init_fn is not None:\n",
    "        if hasattr(module, 'weight'):\n",
    "            init_fn(module.weight)\n",
    "        if hasattr(module, 'bias') and hasattr(module.bias, 'data'):\n",
    "            module.bias.data.fill_(0.)\n",
    "    return module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d(ni, no, kernel=3, stride=1, padding=None, bias=False):\n",
    "    padding = padding or kernel//2\n",
    "    layer = nn.Conv2d(ni, no, kernel, stride, padding, bias=bias)\n",
    "    return init_default(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchnorm2d(nf):\n",
    "    bn = nn.BatchNorm2d(nf)\n",
    "    with torch.no_grad():\n",
    "        bn.bias.fill_(1e-3)\n",
    "        bn.weight.fill_(0)\n",
    "    return bn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    with torch.no_grad():\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            nn.init.kaiming_normal_(m.weight, mode='fan_out')\n",
    "            if hasattr(m, 'bias') and m.bias is not None:\n",
    "                nn.init.zeros_(m.bias)\n",
    "        elif isinstance(m, nn.BatchNorm2d):\n",
    "            nn.init.constant_(m.weight, 1)\n",
    "            nn.init.constant_(m.bias, 1e-3)\n",
    "        elif isinstance(m, nn.Linear):\n",
    "            nn.init.kaiming_normal_(m.weight)\n",
    "            nn.init.zeros_(m.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leaky_linear(ni, no, dropout=None, bn=True):\n",
    "    layers = []\n",
    "    if bn:\n",
    "        layers.append(nn.BatchNorm1d(ni))\n",
    "    if dropout is not None and droupout > 0:\n",
    "        layers.append(nn.Dropout(dropout))\n",
    "    layers.append(nn.Linear(ni, no))\n",
    "    layers.append(nn.LeakyReLU(0.01, True))\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_classes, arch=models.resnet18, init_fn=init_weights):\n",
    "        super().__init__()\n",
    "        \n",
    "        model = arch(True)\n",
    "        seq_model = as_sequential(model)\n",
    "        backbone, classifier = seq_model[:-2], seq_model[-2:]\n",
    "        out_shape = get_output_shape(backbone)\n",
    "        input_size = out_shape[0] * 2\n",
    "        \n",
    "        self.backbone = backbone\n",
    "        self.cat = AdaptiveConcatPool2d()\n",
    "        self.flatten = Flatten()\n",
    "        self.block1 = leaky_linear(input_size, 512)\n",
    "        self.block2 = leaky_linear(512, 256)\n",
    "        self.out = nn.Linear(256, n_classes)\n",
    "        self.init(init_fn)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        x = self.cat(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "    \n",
    "    def init(self, fn=None):\n",
    "        if fn is None:\n",
    "            return\n",
    "        self.apply(fn)                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Classifier(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = torch.zeros((4, 3, 128, 128))\n",
    "out = model(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import functional as F\n",
    "from torch.optim import Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import cpu_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def default(x, fallback=None):\n",
    "    return x if x is not None else fallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_dicts(ds):\n",
    "    merged = OrderedDict()\n",
    "    for d in ds:\n",
    "        for k, v in d.items():\n",
    "            merged[k] = v\n",
    "    return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Callback:\n",
    "    \n",
    "    def training_started(self, **kwargs):\n",
    "        pass\n",
    "    \n",
    "    def training_ended(self, **kwargs):\n",
    "        pass\n",
    "    \n",
    "    def epoch_started(self, **kwargs):\n",
    "        pass\n",
    "    \n",
    "    def phase_started(self, **kwargs):\n",
    "        pass\n",
    "\n",
    "    def phase_ended(self, **kwargs):\n",
    "        pass\n",
    "    \n",
    "    def epoch_ended(self, **kwargs):\n",
    "        pass\n",
    "    \n",
    "    def batch_started(self, **kwargs):\n",
    "        pass\n",
    "    \n",
    "    def batch_ended(self, **kwargs):\n",
    "        pass\n",
    "    \n",
    "    def before_forward_pass(self, **kwargs):\n",
    "        pass\n",
    "    \n",
    "    def after_forward_pass(self, **kwargs):\n",
    "        pass\n",
    "    \n",
    "    def before_backward_pass(self, **kwargs):\n",
    "        pass\n",
    "    \n",
    "    def after_backward_pass(self, **kwargs):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RollingLoss(Callback):\n",
    "    \n",
    "    def __init__(self, smooth=0.98):\n",
    "        self.smooth = smooth\n",
    "    \n",
    "    def batch_ended(self, phase, **kwargs):\n",
    "        prev = phase.rolling_loss\n",
    "        a = self.smooth\n",
    "        avg_loss = a*prev + (1 - a)*phase.batch_loss\n",
    "        debias_loss = avg_loss/(1 - a**phase.batch_index)\n",
    "        phase.rolling_loss = avg_loss\n",
    "        phase.update(debias_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class History(Callback):\n",
    "        \n",
    "    def training_started(self, **kwargs):\n",
    "        self.recorded = None\n",
    "    \n",
    "    def training_ended(self, phases, **kwargs):\n",
    "        metrics = [phase.metrics_history for phase in self.phases]\n",
    "        self.recorded = pd.DataFrame(merge_dicts(metrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StreamLogger(Callback):\n",
    "    \"\"\"\n",
    "    Writes performance metrics collected during the training process into list\n",
    "    of streams.\n",
    "\n",
    "    Parameters:\n",
    "        streams: A list of file-like objects with `write()` method.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, streams=None, log_every=1):\n",
    "        self.streams = streams or [sys.stdout]\n",
    "        self.log_every = log_every\n",
    "    \n",
    "    def epoch_ended(self, phases, epoch, **kwargs):\n",
    "        metrics = merge_dicts([phase.last_metrics for phase in phases])\n",
    "        values = [f'{k}={v:.4f}' for k, v in metrics.items()]\n",
    "        values_string = ', '.join(values)\n",
    "        string = f'Epoch: {epoch:4d} | {values_string}\\n'\n",
    "        for stream in self.streams:\n",
    "            stream.write(string)\n",
    "            stream.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImprovementTracker(Callback):\n",
    "    \n",
    "    def __init__(self, patience=1, metric='valid_loss', better=min):\n",
    "        self.patience = patience\n",
    "        self.metric = metric\n",
    "        self.better = better\n",
    "        self.no_improvement = None\n",
    "        self.stagnation = None\n",
    "        self.best_value = None\n",
    "\n",
    "    @property\n",
    "    def improved(self):\n",
    "        return self.no_improvement == 0\n",
    "        \n",
    "    def training_started(self, **kwargs):\n",
    "        self.no_improvement = 0\n",
    "        self.stagnation = False\n",
    "        \n",
    "    def epoch_ended(self, phases, epoch, **kwargs):\n",
    "        metrics = dict(ChainMap(*[phase.last_metrics for phase in phases]))\n",
    "        value = metrics[self.metric]\n",
    "        best_value = default(self.best_value, value)\n",
    "        improved = self.better(best_value, value) == value\n",
    "        if not improved:\n",
    "            self.no_improvement += 1\n",
    "        else:\n",
    "            self.best_value = value\n",
    "            self.no_improvement = 0\n",
    "        if self.no_improvement >= self.patience:\n",
    "            self.stagnation = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProgressBar(Callback):\n",
    "    \n",
    "    def training_started(self, phases, **kwargs):\n",
    "        bars = OrderedDict()\n",
    "        for phase in phases:\n",
    "            bars[phase.name] = tqdm(total=len(phase.loader), desc=phase.name)\n",
    "        self.bars = bars\n",
    "    \n",
    "    def batch_ended(self, phase, **kwargs):\n",
    "        bar = self.bars[phase.name]\n",
    "        bar.set_postfix_str(f'loss: {phase.last_loss:.4f}')\n",
    "        bar.update(1)\n",
    "        bar.refresh()\n",
    "        \n",
    "    def epoch_ended(self, **kwargs):\n",
    "        for bar in self.bars.values():\n",
    "            bar.n = 0\n",
    "            bar.refresh()\n",
    "            \n",
    "    def training_ended(self, **kwargs):\n",
    "        for bar in self.bars.values():\n",
    "            bar.n = bar.total\n",
    "            bar.refresh()\n",
    "            bar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_snake_case(string):\n",
    "    s = re.sub('(.)([A-Z][a-z]+)', r'\\1_\\2', string)\n",
    "    return re.sub('([a-z0-9])([A-Z])', r'\\1_\\2', s).lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CallbacksGroup:\n",
    "    \n",
    "    def __init__(self, callbacks):\n",
    "        self.callbacks = callbacks\n",
    "        self.named_callbacks = {\n",
    "            to_snake_case(cb.__class__.__name__): cb\n",
    "            for cb in callbacks}\n",
    "    \n",
    "    def set_loop(self, loop):\n",
    "        self.loop = loop\n",
    "        \n",
    "    def __getitem__(self, item):\n",
    "        item = to_snake_case(item)\n",
    "        if item in self.named_callbacks:\n",
    "            return self.named_callbacks[item] \n",
    "        raise KeyError(f'callback name not found: {item}')\n",
    "        \n",
    "    def training_started(self, **kwargs):\n",
    "        for cb in self.callbacks:\n",
    "            cb.training_started(**kwargs)\n",
    "    \n",
    "    def training_ended(self, **kwargs):\n",
    "        for cb in self.callbacks:\n",
    "            cb.training_ended(**kwargs)\n",
    "            \n",
    "    def phase_started(self, **kwargs):\n",
    "        for cb in self.callbacks:\n",
    "            cb.phase_started(**kwargs)\n",
    "\n",
    "    def phase_ended(self, **kwargs):\n",
    "        for cb in self.callbacks:\n",
    "            cb.phase_ended(**kwargs)\n",
    "    \n",
    "    def epoch_started(self, **kwargs):\n",
    "        for cb in self.callbacks:\n",
    "            cb.epoch_started(**kwargs)\n",
    "    \n",
    "    def epoch_ended(self, **kwargs):\n",
    "        for cb in self.callbacks:\n",
    "            cb.epoch_ended(**kwargs)\n",
    "    \n",
    "    def batch_started(self, **kwargs):\n",
    "        for cb in self.callbacks:\n",
    "            cb.batch_started(**kwargs)\n",
    "            \n",
    "    def batch_ended(self, **kwargs):\n",
    "        for cb in self.callbacks:\n",
    "            cb.batch_ended(**kwargs)\n",
    "    \n",
    "    def before_forward_pass(self, **kwargs):\n",
    "        for cb in self.callbacks:\n",
    "            cb.before_forward_pass(**kwargs)\n",
    "    \n",
    "    def after_forward_pass(self, **kwargs):\n",
    "        for cb in self.callbacks:\n",
    "            cb.after_forward_pass(**kwargs)\n",
    "    \n",
    "    def before_backward_pass(self, **kwargs):\n",
    "        for cb in self.callbacks:\n",
    "            cb.before_forward_pass(**kwargs)\n",
    "    \n",
    "    def after_backward_pass(self, **kwargs):\n",
    "        for cb in self.callbacks:\n",
    "            cb.after_backward_pass(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Phase:\n",
    "    \"\"\"\n",
    "    Model training loop phase.\n",
    "\n",
    "    Each model's training loop iteration could be separated into (at least) two\n",
    "    phases: training and validation. The instances of this class track\n",
    "    metrics and counters, related to the specific phase, and keep the reference\n",
    "    to subset of data, used during phase.\n",
    "    \"\"\"\n",
    "    def __init__(self, name: str, loader: DataLoader, grad: bool=True):\n",
    "        self.name = name\n",
    "        self.loader = loader\n",
    "        self.grad = grad\n",
    "        self.batch_loss = None\n",
    "        self.batch_index = 0\n",
    "        self.rolling_loss = 0\n",
    "        self.losses = []\n",
    "        \n",
    "    @property\n",
    "    def last_loss(self):\n",
    "        return self.losses[-1] if self.losses else None\n",
    "    \n",
    "    @property\n",
    "    def last_metrics(self):\n",
    "        return {f'{self.name}_loss': self.last_loss}\n",
    "    \n",
    "    @property\n",
    "    def metrics_history(self):\n",
    "        return {f'{self.name}_loss': self.losses}\n",
    "        \n",
    "    def update(self, loss):\n",
    "        self.losses.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExpandChannels:\n",
    "    \n",
    "    def __init__(self, num_of_channels=3):\n",
    "        self.nc = num_of_channels\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        return x.expand((self.nc,) + x.shape[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transforms():\n",
    "    return Compose([\n",
    "        ToTensor(), \n",
    "        ExpandChannels(3),\n",
    "        Normalize((0.1307,), (0.3081,))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def place_and_unwrap(batch, dev):\n",
    "    x, *y = batch\n",
    "    x = x.to(dev)\n",
    "    y = [tensor.to(dev) for tensor in y]\n",
    "    if len(y) == 1:\n",
    "        [y] = y\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad0188a0f8924f15ae918c4447356190",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='train', max=30), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b15983a6ee5e4c558341165cbcdb0a6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='valid', max=5), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:    0 | train_loss=1.7084, valid_loss=0.6339\n",
      "Epoch:    1 | train_loss=1.0671, valid_loss=0.4370\n",
      "Epoch:    2 | train_loss=0.7400, valid_loss=0.3379\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n = 4\n",
    "n_jobs = default(n, cpu_count())\n",
    "epochs = 3\n",
    "batch_size = 2048\n",
    "device = torch.device('cuda:1')\n",
    "\n",
    "transforms = get_transforms()\n",
    "root = Path('~/data/mnist').expanduser()\n",
    "train_ds = MNIST(root, train=True, download=True, transform=transforms)\n",
    "valid_ds = MNIST(root, train=False, transform=transforms)\n",
    "\n",
    "phases = [\n",
    "    Phase('train', DataLoader(train_ds, batch_size, shuffle=True)),\n",
    "    Phase('valid', DataLoader(valid_ds, batch_size), grad=False)\n",
    "]\n",
    "\n",
    "# model = Classifier(10)\n",
    "model = Net()\n",
    "model.to(device)\n",
    "opt = optim.Adam(model.parameters(), lr=1e-3)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "cb = CallbacksGroup([RollingLoss(), StreamLogger(), ProgressBar()])\n",
    "cb.training_started(phases=phases)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    cb.epoch_started(epoch=epoch)\n",
    "\n",
    "    for phase in phases:\n",
    "        n = len(phase.loader)\n",
    "        cb.phase_started(phase=phase, total_batches=n)\n",
    "        is_training = phase.grad\n",
    "        model.train(is_training)\n",
    "        \n",
    "        for batch in phase.loader:\n",
    "\n",
    "            phase.batch_index += 1\n",
    "            cb.batch_started(phase=phase, total_batches=n)\n",
    "            x, y = place_and_unwrap(batch, device)\n",
    "            \n",
    "            with torch.set_grad_enabled(is_training):\n",
    "                cb.before_forward_pass()\n",
    "                out = model(x)\n",
    "                cb.after_forward_pass()\n",
    "                loss = loss_fn(out, y)\n",
    "            \n",
    "            if is_training:\n",
    "                opt.zero_grad()\n",
    "                cb.before_backward_pass()\n",
    "                loss.backward()\n",
    "                cb.after_backward_pass()\n",
    "                opt.step()\n",
    "            \n",
    "            phase.batch_loss = loss.item()\n",
    "            cb.batch_ended(phase=phase, output=out, target=y)\n",
    "            \n",
    "        cb.phase_ended(phase=phase)\n",
    "    \n",
    "    cb.epoch_ended(phases=phases, epoch=epoch)\n",
    "    \n",
    "cb.training_ended(phases=phases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
