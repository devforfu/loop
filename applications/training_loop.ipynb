{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Training Loop Example\n",
    "\n",
    "The notebook contains an example of simple training loop initialization and training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torchvision import models\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import Compose, ToTensor, Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "try:\n",
    "    old_path\n",
    "except NameError:\n",
    "    old_path = sys.path.copy()\n",
    "    sys.path = [Path.cwd().parent.as_posix()] + old_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from loop.callbacks import History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flat_model(model):\n",
    "    \"\"\"Converts model with nested modules into single list of modules\"\"\"\n",
    "    \n",
    "    def flatten(m):\n",
    "        children = list(m.children())\n",
    "        if not children:\n",
    "            return [m]\n",
    "        return sum([flatten(child) for child in children], [])\n",
    "    \n",
    "    return nn.Sequential(*flatten(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def as_sequential(model):\n",
    "    return nn.Sequential(*list(model.children()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_output_shape(model):\n",
    "    \"\"\"Pass a dummy input through the sequential model to get the output tensor shape.\"\"\"\n",
    "    first, *rest = flat_model(model)\n",
    "    shape = first.in_channels, 128, 128\n",
    "    dummy_input = torch.zeros(shape)\n",
    "    out = model(dummy_input[None])\n",
    "    return list(out.size())[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaptiveConcatPool2d(nn.Module):\n",
    "    \n",
    "    def __init__(self, size=1):\n",
    "        super().__init__()\n",
    "        self.avg = nn.AdaptiveAvgPool2d(size)\n",
    "        self.max = nn.AdaptiveMaxPool2d(size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return torch.cat([self.max(x), self.avg(x)], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x.view(x.size(0), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_default(module, init_fn=nn.init.kaiming_normal_):\n",
    "    if init_fn is not None:\n",
    "        if hasattr(module, 'weight'):\n",
    "            init_fn(module.weight)\n",
    "        if hasattr(module, 'bias') and hasattr(module.bias, 'data'):\n",
    "            module.bias.data.fill_(0.)\n",
    "    return module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d(ni, no, kernel=3, stride=1, padding=None, bias=False):\n",
    "    padding = padding or kernel//2\n",
    "    layer = nn.Conv2d(ni, no, kernel, stride, padding, bias=bias)\n",
    "    return init_default(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchnorm2d(nf):\n",
    "    bn = nn.BatchNorm2d(nf)\n",
    "    with torch.no_grad():\n",
    "        bn.bias.fill_(1e-3)\n",
    "        bn.weight.fill_(0)\n",
    "    return bn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    with torch.no_grad():\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            nn.init.kaiming_normal_(m.weight, mode='fan_out')\n",
    "            if hasattr(m, 'bias') and m.bias is not None:\n",
    "                nn.init.zeros_(m.bias)\n",
    "        elif isinstance(m, nn.BatchNorm2d):\n",
    "            nn.init.constant_(m.weight, 1)\n",
    "            nn.init.constant_(m.bias, 1e-3)\n",
    "        elif isinstance(m, nn.Linear):\n",
    "            nn.init.kaiming_normal_(m.weight)\n",
    "            nn.init.zeros_(m.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leaky_linear(ni, no, dropout=None, bn=True):\n",
    "    layers = []\n",
    "    if bn:\n",
    "        layers.append(nn.BatchNorm1d(ni))\n",
    "    if dropout is not None and droupout > 0:\n",
    "        layers.append(nn.Dropout(dropout))\n",
    "    layers.append(nn.Linear(ni, no))\n",
    "    layers.append(nn.LeakyReLU(0.01, True))\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_classes, arch=models.resnet18, init_fn=init_weights):\n",
    "        super().__init__()\n",
    "        \n",
    "        model = arch(True)\n",
    "        seq_model = as_sequential(model)\n",
    "        backbone, classifier = seq_model[:-2], seq_model[-2:]\n",
    "        out_shape = get_output_shape(backbone)\n",
    "        input_size = out_shape[0] * 2\n",
    "        \n",
    "        self.backbone = backbone\n",
    "        self.cat = AdaptiveConcatPool2d()\n",
    "        self.flatten = Flatten()\n",
    "        self.block1 = leaky_linear(input_size, 512)\n",
    "        self.block2 = leaky_linear(512, 256)\n",
    "        self.out = nn.Linear(256, n_classes)\n",
    "        self.init(init_fn)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        x = self.cat(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "    \n",
    "    def init(self, fn=None):\n",
    "        if fn is None:\n",
    "            return\n",
    "        self.apply(fn)                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Classifier(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = torch.zeros((4, 3, 128, 128))\n",
    "out = model(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import functional as F\n",
    "from torch.optim import Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import cpu_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def default(x, fallback=None):\n",
    "    return x if x is not None else fallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Callback:\n",
    "    \n",
    "    def training_started(self):\n",
    "        pass\n",
    "    \n",
    "    def training_ended(self):\n",
    "        pass\n",
    "    \n",
    "    def epoch_started(self, **kwargs):\n",
    "        pass\n",
    "    \n",
    "    def phase_started(self, **kwargs):\n",
    "        pass\n",
    "\n",
    "    def phase_ended(self, **kwargs):\n",
    "        pass\n",
    "    \n",
    "    def epoch_ended(self, **kwargs):\n",
    "        pass\n",
    "    \n",
    "    def batch_started(self, **kwargs):\n",
    "        pass\n",
    "    \n",
    "    def batch_ended(self, **kwargs):\n",
    "        pass\n",
    "    \n",
    "    def before_forward_pass(self, **kwargs):\n",
    "        pass\n",
    "    \n",
    "    def after_forward_pass(self, **kwargs):\n",
    "        pass\n",
    "    \n",
    "    def before_backward_pass(self, **kwargs):\n",
    "        pass\n",
    "    \n",
    "    def after_backward_pass(self, **kwargs):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RollingLoss(Callback):\n",
    "    \n",
    "    def __init__(self, smooth=0.98):\n",
    "        self.smooth = smooth\n",
    "    \n",
    "    def batch_ended(self, phase, **kwargs):\n",
    "        prev = phase.rolling_loss\n",
    "        a = self.smooth\n",
    "        avg_loss = a*prev + (1 - a)*phase.batch_loss\n",
    "        debias_loss = avg_loss/(1 - a**phase.batch_index)\n",
    "        phase.rolling_loss = avg_loss\n",
    "        phase.last_loss = debias_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StreamLogger(Callback):\n",
    "    \"\"\"\n",
    "    Writes performance metrics collected during the training process into list\n",
    "    of streams.\n",
    "\n",
    "    Parameters:\n",
    "        streams: A list of file-like objects with `write()` method.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, streams=None, log_every=1):\n",
    "        self.streams = streams or [sys.stdout]\n",
    "        self.log_every = log_every\n",
    "    \n",
    "    def epoch_ended(self, phases, epoch, **kwargs):\n",
    "        losses = [f'{phase.name}={phase.last_loss:.4f}' for phase in phases]\n",
    "        loss_string = ', '.join(losses)\n",
    "        string = f'Epoch: {epoch:4d} | {loss_string}\\n'\n",
    "        for stream in self.streams:\n",
    "            stream.write(string)\n",
    "            stream.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProgressBar(Callback):\n",
    "    \n",
    "    def phase_started(self, phase, total_batches, **kwargs):\n",
    "        if not hasattr(self, 'bar'):\n",
    "            self.bar = tqdm(total=total_batches, desc=phase.name)\n",
    "        else:\n",
    "            self.bar.n = 0\n",
    "            self.bar.total = total_batches\n",
    "            self.bar.desc = phase.name\n",
    "        self.bar.refresh()\n",
    "    \n",
    "    def batch_ended(self, phase, **kwargs):\n",
    "        self.bar.set_postfix_str(f'loss: {phase.last_loss:.4f}')\n",
    "        self.bar.update(1)\n",
    "        \n",
    "    def trianing_ended(self, **kwargs):\n",
    "        self.bar.close()\n",
    "        self.bar = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CallbacksGroup:\n",
    "    \n",
    "    def __init__(self, callbacks):\n",
    "        self.callbacks = callbacks\n",
    "        self.named_callbacks = {cb.__class__.__name__: cb for cb in callbacks}\n",
    "    \n",
    "    def set_loop(self, loop):\n",
    "        self.loop = loop\n",
    "        \n",
    "    def training_started(self, **kwargs):\n",
    "        for cb in self.callbacks:\n",
    "            cb.training_started()\n",
    "    \n",
    "    def training_ended(self, **kwargs):\n",
    "        for cb in self.callbacks:\n",
    "            cb.training_ended()\n",
    "            \n",
    "    def phase_started(self, **kwargs):\n",
    "        for cb in self.callbacks:\n",
    "            cb.phase_started(**kwargs)\n",
    "\n",
    "    def phase_ended(self, **kwargs):\n",
    "        for cb in self.callbacks:\n",
    "            cb.phase_ended(**kwargs)\n",
    "    \n",
    "    def epoch_started(self, **kwargs):\n",
    "        for cb in self.callbacks:\n",
    "            cb.epoch_started(**kwargs)\n",
    "    \n",
    "    def epoch_ended(self, **kwargs):\n",
    "        for cb in self.callbacks:\n",
    "            cb.epoch_ended(**kwargs)\n",
    "    \n",
    "    def batch_started(self, **kwargs):\n",
    "        for cb in self.callbacks:\n",
    "            cb.batch_started(**kwargs)\n",
    "            \n",
    "    def batch_ended(self, **kwargs):\n",
    "        for cb in self.callbacks:\n",
    "            cb.batch_ended(**kwargs)\n",
    "    \n",
    "    def before_forward_pass(self, **kwargs):\n",
    "        for cb in self.callbacks:\n",
    "            cb.before_forward_pass(**kwargs)\n",
    "    \n",
    "    def after_forward_pass(self, **kwargs):\n",
    "        for cb in self.callbacks:\n",
    "            cb.after_forward_pass(**kwargs)\n",
    "    \n",
    "    def before_backward_pass(self, **kwargs):\n",
    "        for cb in self.callbacks:\n",
    "            cb.before_forward_pass(**kwargs)\n",
    "    \n",
    "    def after_backward_pass(self, **kwargs):\n",
    "        for cb in self.callbacks:\n",
    "            cb.after_backward_pass(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Phase:\n",
    "    \"\"\"\n",
    "    Model training loop phase.\n",
    "\n",
    "    Each model's training loop iteration could be separated into (at least) two\n",
    "    phases: training and validation. The instances of this class track\n",
    "    metrics and counters, related to the specific phase, and keep the reference\n",
    "    to subset of data, used during phase.\n",
    "    \"\"\"\n",
    "    def __init__(self, name: str, loader: DataLoader, grad: bool=True):\n",
    "        self.name = name\n",
    "        self.loader = loader\n",
    "        self.grad = grad\n",
    "        self.batch_loss = None\n",
    "        self.batch_index = 0\n",
    "        self.rolling_loss = 0\n",
    "        self.last_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExpandChannels:\n",
    "    \n",
    "    def __init__(self, num_of_channels=3):\n",
    "        self.nc = num_of_channels\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        return x.expand((self.nc,) + x.shape[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transforms():\n",
    "    return Compose([\n",
    "        ToTensor(), \n",
    "        ExpandChannels(3),\n",
    "        Normalize((0.1307,), (0.3081,))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def place_and_unwrap(batch, dev):\n",
    "    x, *y = batch\n",
    "    x = x.to(dev)\n",
    "    y = [tensor.to(dev) for tensor in y]\n",
    "    if len(y) == 1:\n",
    "        [y] = y\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ddae4044ed843ec9754472eab78090e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='train', max=15), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:    0 | train=0.3807, valid=4.4185\n",
      "Epoch:    1 | train=0.1899, valid=2.2400\n",
      "Epoch:    2 | train=0.1181, valid=1.4651\n",
      "Epoch:    3 | train=0.0798, valid=1.0762\n",
      "Epoch:    4 | train=0.0563, valid=0.8417\n",
      "Epoch:    5 | train=0.0407, valid=0.6880\n",
      "Epoch:    6 | train=0.0302, valid=0.5790\n",
      "Epoch:    7 | train=0.0227, valid=0.4964\n",
      "Epoch:    8 | train=0.0178, valid=0.4346\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-183-89f955e89cfb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m             \u001b[0mphase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m             \u001b[0mcb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_ended\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphase\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mphase\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n = 4\n",
    "n_jobs = default(n, cpu_count())\n",
    "epochs = 10\n",
    "batch_size = 4096\n",
    "device = torch.device('cuda:1')\n",
    "\n",
    "transforms = get_transforms()\n",
    "root = Path('~/data/mnist').expanduser()\n",
    "train_ds = MNIST(root, train=True, download=True, transform=transforms)\n",
    "valid_ds = MNIST(root, train=False, transform=transforms)\n",
    "\n",
    "phases = [\n",
    "    Phase('train', DataLoader(train_ds, batch_size, shuffle=True)),\n",
    "    Phase('valid', DataLoader(valid_ds, batch_size), grad=False)\n",
    "]\n",
    "\n",
    "model = Classifier(10)\n",
    "model.to(device)\n",
    "opt = optim.Adam(model.parameters())\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "cb = CallbacksGroup([RollingLoss(), StreamLogger(), ProgressBar()])\n",
    "cb.training_started()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    cb.epoch_started(epoch=epoch)\n",
    "\n",
    "    for phase in phases:\n",
    "        n = len(phase.loader)\n",
    "        cb.phase_started(phase=phase, total_batches=n)\n",
    "        is_training = phase.grad\n",
    "        model.train(is_training)\n",
    "        \n",
    "        for batch in phase.loader:\n",
    "\n",
    "            phase.batch_index += 1\n",
    "            cb.batch_started(phase=phase, total_batches=n)\n",
    "            x, y = place_and_unwrap(batch, device)\n",
    "            \n",
    "            with torch.set_grad_enabled(is_training):\n",
    "                cb.before_forward_pass()\n",
    "                out = model(x)\n",
    "                cb.after_forward_pass()\n",
    "                loss = loss_fn(out, y)\n",
    "            \n",
    "            if is_training:\n",
    "                opt.zero_grad()\n",
    "                cb.before_backward_pass()\n",
    "                loss.backward()\n",
    "                cb.after_backward_pass()\n",
    "                opt.step()\n",
    "            \n",
    "            phase.batch_loss = loss.item()\n",
    "            cb.batch_ended(phase=phase, output=out, target=y)\n",
    "            \n",
    "        cb.phase_ended(phase=phase)\n",
    "    \n",
    "    cb.epoch_ended(phases=phases, epoch=epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
