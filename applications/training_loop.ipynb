{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Training Loop Example\n",
    "\n",
    "The notebook contains an example of simple training loop initialization and training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torchvision import models\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import Compose, ToTensor, Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ck/anaconda3/envs/fastai/lib/python3.7/site-packages/tqdm/autonotebook/__init__.py:14: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  \" (e.g. in jupyter console)\", TqdmExperimentalWarning)\n"
     ]
    }
   ],
   "source": [
    "from tqdm.autonotebook import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "try:\n",
    "    old_path\n",
    "except NameError:\n",
    "    old_path = sys.path.copy()\n",
    "    sys.path = [Path.cwd().parent.as_posix()] + old_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "from collections import ChainMap, OrderedDict, defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from loop.callbacks import History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flat_model(model):\n",
    "    \"\"\"Converts model with nested modules into single list of modules\"\"\"\n",
    "    \n",
    "    def flatten(m):\n",
    "        children = list(m.children())\n",
    "        if not children:\n",
    "            return [m]\n",
    "        return sum([flatten(child) for child in children], [])\n",
    "    \n",
    "    return nn.Sequential(*flatten(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def as_sequential(model):\n",
    "    return nn.Sequential(*list(model.children()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_output_shape(model):\n",
    "    \"\"\"Pass a dummy input through the sequential model to get the output tensor shape.\"\"\"\n",
    "    first, *rest = flat_model(model)\n",
    "    shape = first.in_channels, 128, 128\n",
    "    dummy_input = torch.zeros(shape)\n",
    "    out = model(dummy_input[None])\n",
    "    return list(out.size())[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaptiveConcatPool2d(nn.Module):\n",
    "    \n",
    "    def __init__(self, size=1):\n",
    "        super().__init__()\n",
    "        self.avg = nn.AdaptiveAvgPool2d(size)\n",
    "        self.max = nn.AdaptiveMaxPool2d(size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return torch.cat([self.max(x), self.avg(x)], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x.view(x.size(0), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_default(module, init_fn=nn.init.kaiming_normal_):\n",
    "    if init_fn is not None:\n",
    "        if hasattr(module, 'weight'):\n",
    "            init_fn(module.weight)\n",
    "        if hasattr(module, 'bias') and hasattr(module.bias, 'data'):\n",
    "            module.bias.data.fill_(0.)\n",
    "    return module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d(ni, no, kernel=3, stride=1, padding=None, bias=False):\n",
    "    padding = padding or kernel//2\n",
    "    layer = nn.Conv2d(ni, no, kernel, stride, padding, bias=bias)\n",
    "    return init_default(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchnorm2d(nf):\n",
    "    bn = nn.BatchNorm2d(nf)\n",
    "    with torch.no_grad():\n",
    "        bn.bias.fill_(1e-3)\n",
    "        bn.weight.fill_(0)\n",
    "    return bn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    name = m.__class__.__name__\n",
    "    with torch.no_grad():\n",
    "        if name.find('Conv') != -1:\n",
    "            nn.init.kaiming_normal_(m.weight, mode='fan_out')\n",
    "            if hasattr(m, 'bias') and m.bias is not None:\n",
    "                nn.init.zeros_(m.bias)\n",
    "        elif name.find('BatchNorm') != -1:\n",
    "            nn.init.constant_(m.weight, 1)\n",
    "            nn.init.constant_(m.bias, 1e-3)\n",
    "        elif name.find('Linear') != -1:\n",
    "            nn.init.kaiming_normal_(m.weight)\n",
    "            nn.init.zeros_(m.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leaky_linear(ni, no, dropout=None, bn=True):\n",
    "    layers = []\n",
    "    if bn:\n",
    "        layers.append(nn.BatchNorm1d(ni))\n",
    "    if dropout is not None and dropout > 0:\n",
    "        layers.append(nn.Dropout(dropout))\n",
    "    layers.append(nn.Linear(ni, no))\n",
    "    layers.append(nn.LeakyReLU(0.01, True))\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_classes, arch=models.resnet18, init_fn=init_weights):\n",
    "        super().__init__()\n",
    "        \n",
    "        model = arch(True)\n",
    "        seq_model = as_sequential(model)\n",
    "        backbone, classifier = seq_model[:-2], seq_model[-2:]\n",
    "        out_shape = get_output_shape(backbone)\n",
    "        input_size = out_shape[0] * 2\n",
    "        \n",
    "        self.backbone = backbone\n",
    "        self.top = nn.Sequential(\n",
    "            AdaptiveConcatPool2d(),\n",
    "            Flatten(),\n",
    "            leaky_linear(input_size, 512, 0.25),\n",
    "            leaky_linear(512, 256, 0.5),\n",
    "            nn.Linear(256, n_classes)\n",
    "        )\n",
    "        \n",
    "        self.init(init_fn)\n",
    "        \n",
    "    def freeze_backbone(self, freeze=True, bn=True):\n",
    "        for child in self.backbone.children():\n",
    "            name = child.__class__.__name__\n",
    "            if not bn and name.find('BatchNorm') != -1:\n",
    "                continue\n",
    "            for p in child.parameters():\n",
    "                p.requires_grad = not freeze\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.top(self.backbone(x))\n",
    "    \n",
    "    def init(self, fn=None):\n",
    "        if fn is None:\n",
    "            return\n",
    "        self.top.apply(fn)                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import functional as F\n",
    "from torch.optim import Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import cpu_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def default(x, fallback=None):\n",
    "    return x if x is not None else fallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_dicts(ds):\n",
    "    merged = OrderedDict()\n",
    "    for d in ds:\n",
    "        for k, v in d.items():\n",
    "            merged[k] = v\n",
    "    return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Callback:\n",
    "    \n",
    "    def training_started(self, **kwargs):\n",
    "        pass\n",
    "    \n",
    "    def training_ended(self, **kwargs):\n",
    "        pass\n",
    "    \n",
    "    def epoch_started(self, **kwargs):\n",
    "        pass\n",
    "    \n",
    "    def phase_started(self, **kwargs):\n",
    "        pass\n",
    "\n",
    "    def phase_ended(self, **kwargs):\n",
    "        pass\n",
    "    \n",
    "    def epoch_ended(self, **kwargs):\n",
    "        pass\n",
    "    \n",
    "    def batch_started(self, **kwargs):\n",
    "        pass\n",
    "    \n",
    "    def batch_ended(self, **kwargs):\n",
    "        pass\n",
    "    \n",
    "    def before_forward_pass(self, **kwargs):\n",
    "        pass\n",
    "    \n",
    "    def after_forward_pass(self, **kwargs):\n",
    "        pass\n",
    "    \n",
    "    def before_backward_pass(self, **kwargs):\n",
    "        pass\n",
    "    \n",
    "    def after_backward_pass(self, **kwargs):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RollingLoss(Callback):\n",
    "    \n",
    "    def __init__(self, smooth=0.98):\n",
    "        self.smooth = smooth\n",
    "    \n",
    "    def batch_ended(self, phase, **kwargs):\n",
    "        prev = phase.rolling_loss\n",
    "        a = self.smooth\n",
    "        avg_loss = a*prev + (1 - a)*phase.batch_loss\n",
    "        debias_loss = avg_loss/(1 - a**phase.batch_index)\n",
    "        phase.rolling_loss = avg_loss\n",
    "        phase.update(debias_loss)\n",
    "        \n",
    "    def epoch_ended(self, phases, **kwargs):\n",
    "        for phase in phases:\n",
    "            phase.update_metric('loss', phase.last_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(out, y_true):\n",
    "    y_hat = out.argmax(dim=-1).view(y_true.size(0), -1)\n",
    "    y_true = y_true.view(y_true.size(0), -1)\n",
    "    match = y_hat == y_true\n",
    "    return match.float().mean()\n",
    "\n",
    "\n",
    "class Accuracy(Callback):\n",
    "    \n",
    "    def epoch_started(self, **kwargs):\n",
    "        self.values = defaultdict(int)\n",
    "        self.counts = defaultdict(int)\n",
    "        \n",
    "    def batch_ended(self, phase, output, target, **kwargs):\n",
    "        acc = accuracy(output, target).detach().item()\n",
    "        self.counts[phase.name] += target.size(0)\n",
    "        self.values[phase.name] += target.size(0) * acc\n",
    "    \n",
    "    def epoch_ended(self, phases, **kwargs):\n",
    "        for phase in phases:\n",
    "            metric = self.values[phase.name]/self.counts[phase.name]\n",
    "            phase.update_metric('accuracy', metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class History(Callback):\n",
    "        \n",
    "    def training_started(self, **kwargs):\n",
    "        self.recorded = None\n",
    "        self.epochs = []\n",
    "        \n",
    "    def epoch_ended(self, epoch, **kwargs):\n",
    "        self.epochs.append(epoch)\n",
    "    \n",
    "    def training_ended(self, phases, **kwargs):\n",
    "        epochs = {'epoch': np.array(self.epochs).astype(int)}\n",
    "        metrics = [epochs] + [phase.metrics_history for phase in phases]\n",
    "        data = pd.DataFrame(merge_dicts(metrics))\n",
    "        data.reset_index(inplace=True, drop=True)\n",
    "        self.recorded = data\n",
    "        \n",
    "    def plot(self, metrics, ax=None):\n",
    "        self.recorded.plot(x='epoch', y=metrics, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StreamLogger(Callback):\n",
    "    \"\"\"\n",
    "    Writes performance metrics collected during the training process into list\n",
    "    of streams.\n",
    "\n",
    "    Parameters:\n",
    "        streams: A list of file-like objects with `write()` method.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, streams=None, log_every=1):\n",
    "        self.streams = streams or [sys.stdout]\n",
    "        self.log_every = log_every\n",
    "    \n",
    "    def epoch_ended(self, phases, epoch, **kwargs):\n",
    "        metrics = merge_dicts([phase.last_metrics for phase in phases])\n",
    "        values = [f'{k}={v:.4f}' for k, v in metrics.items()]\n",
    "        values_string = ', '.join(values)\n",
    "        string = f'Epoch: {epoch:4d} | {values_string}\\n'\n",
    "        for stream in self.streams:\n",
    "            stream.write(string)\n",
    "            stream.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImprovementTracker(Callback):\n",
    "    \n",
    "    def __init__(self, patience=1, metric='valid_loss', better=min):\n",
    "        self.patience = patience\n",
    "        self.metric = metric\n",
    "        self.better = better\n",
    "        self.no_improvement = None\n",
    "        self.stagnation = None\n",
    "        self.best_value = None\n",
    "\n",
    "    @property\n",
    "    def improved(self):\n",
    "        return self.no_improvement == 0\n",
    "        \n",
    "    def training_started(self, **kwargs):\n",
    "        self.no_improvement = 0\n",
    "        self.stagnation = False\n",
    "        \n",
    "    def epoch_ended(self, phases, epoch, **kwargs):\n",
    "        metrics = merge_dicts([phase.last_metrics for phase in phases])\n",
    "        value = metrics[self.metric]\n",
    "        best_value = default(self.best_value, value)\n",
    "        improved = self.better(best_value, value) == value\n",
    "        if not improved:\n",
    "            self.no_improvement += 1\n",
    "        else:\n",
    "            self.best_value = value\n",
    "            self.no_improvement = 0\n",
    "        if self.no_improvement >= self.patience:\n",
    "            self.stagnation = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProgressBar(Callback):\n",
    "    \n",
    "    def training_started(self, phases, **kwargs):\n",
    "        bars = OrderedDict()\n",
    "        for phase in phases:\n",
    "            bars[phase.name] = tqdm(total=len(phase.loader), desc=phase.name)\n",
    "        self.bars = bars\n",
    "    \n",
    "    def batch_ended(self, phase, **kwargs):\n",
    "        bar = self.bars[phase.name]\n",
    "        bar.set_postfix_str(f'loss: {phase.last_loss:.4f}')\n",
    "        bar.update(1)\n",
    "        bar.refresh()\n",
    "        \n",
    "    def epoch_ended(self, **kwargs):\n",
    "        for bar in self.bars.values():\n",
    "            bar.n = 0\n",
    "            bar.refresh()\n",
    "            \n",
    "    def training_ended(self, **kwargs):\n",
    "        for bar in self.bars.values():\n",
    "            bar.n = bar.total\n",
    "            bar.refresh()\n",
    "            bar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scheduler(Callback):\n",
    "    \n",
    "    default = [{'name': 'lr'}]\n",
    "    \n",
    "    def __init__(self, schedule, mode='epoch', params_conf=None):\n",
    "        self.schedule = schedule\n",
    "        self.params_conf = params_conf or self.default\n",
    "        self.mode = mode\n",
    "        self.history = []\n",
    "        \n",
    "    def training_started(self, optimizer, **kwargs):\n",
    "        self.updater = ParameterUpdater(self.schedule, self.params_conf, optimizer)\n",
    "        self.updater.save_start_values()\n",
    "        \n",
    "    def batch_ended(self, phase, **kwargs):\n",
    "        if self.mode == 'batch' and phase.grad:\n",
    "            self.update_parameters()\n",
    "    \n",
    "    def epoch_ended(self, epoch, **kwargs):\n",
    "        if self.mode == 'epoch':\n",
    "            self.update_parameters()\n",
    "            \n",
    "    def update_parameters(self):\n",
    "        self.history.append(self.updater.current_values())\n",
    "        self.updater.step()\n",
    "    \n",
    "\n",
    "class ParameterUpdater:\n",
    "\n",
    "    def __init__(self, schedule, params, opt=None):\n",
    "        self.schedule = schedule\n",
    "        self.params = params\n",
    "        self.opt = opt\n",
    "        self.start_parameters = None\n",
    "\n",
    "    def set_optimizer(self, opt):\n",
    "        self.opt = opt\n",
    "        \n",
    "    def save_start_values(self):\n",
    "        start = []\n",
    "        for group in self.opt.param_groups:\n",
    "            params = {}\n",
    "            for item in self.params:\n",
    "                name = item['name']\n",
    "                if name in group:\n",
    "                    params[name] = group[name]\n",
    "            start.append(params)\n",
    "        self.start_parameters = start\n",
    "\n",
    "    def current_values(self):\n",
    "        return [\n",
    "            {conf['name']: group[conf['name']]\n",
    "             for conf in self.params}\n",
    "            for group in self.opt.param_groups]\n",
    "\n",
    "    def step(self):\n",
    "        mult = self.schedule.update()\n",
    "        for i, group in enumerate(self.opt.param_groups):\n",
    "            for item in self.params:\n",
    "                name = item['name']\n",
    "                if name in group:\n",
    "                    params = self.start_parameters[i]\n",
    "                    inverse = item.get('inverse', False)\n",
    "                    start_value = params.get(name)\n",
    "                    group[name] = start_value * ((1 - mult) if inverse else mult)\n",
    "\n",
    "                    \n",
    "class CosineAnnealingSchedule:\n",
    "    \"\"\"\n",
    "    The schedule class that returns eta multiplier in range from 0.0 to 1.0.\n",
    "    \"\"\"\n",
    "    def __init__(self, eta_min=0.0, eta_max=1.0, t_max=100, t_mult=2):\n",
    "        self.eta_min = eta_min\n",
    "        self.eta_max = eta_max\n",
    "        self.t_max = t_max\n",
    "        self.t_mult = t_mult\n",
    "        self.iter = 0\n",
    "    \n",
    "    def update(self, **kwargs):\n",
    "        self.iter += 1\n",
    "\n",
    "        eta_min, eta_max, t_max = self.eta_min, self.eta_max, self.t_max\n",
    "\n",
    "        t = self.iter % t_max\n",
    "        eta = eta_min + 0.5*(eta_max - eta_min)*(1 + math.cos(math.pi*t/t_max))\n",
    "        if t == 0:\n",
    "            self.iter = 0\n",
    "            self.t_max *= self.t_mult\n",
    "\n",
    "        return eta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_snake_case(string):\n",
    "    s = re.sub('(.)([A-Z][a-z]+)', r'\\1_\\2', string)\n",
    "    return re.sub('([a-z0-9])([A-Z])', r'\\1_\\2', s).lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CallbacksGroup:\n",
    "    \n",
    "    def __init__(self, callbacks):\n",
    "        self.callbacks = callbacks\n",
    "        self.named_callbacks = {\n",
    "            to_snake_case(cb.__class__.__name__): cb\n",
    "            for cb in callbacks}\n",
    "    \n",
    "    def set_loop(self, loop):\n",
    "        self.loop = loop\n",
    "        \n",
    "    def __getitem__(self, item):\n",
    "        item = to_snake_case(item)\n",
    "        if item in self.named_callbacks:\n",
    "            return self.named_callbacks[item] \n",
    "        raise KeyError(f'callback name not found: {item}')\n",
    "        \n",
    "    def training_started(self, **kwargs):\n",
    "        for cb in self.callbacks:\n",
    "            cb.training_started(**kwargs)\n",
    "    \n",
    "    def training_ended(self, **kwargs):\n",
    "        for cb in self.callbacks:\n",
    "            cb.training_ended(**kwargs)\n",
    "            \n",
    "    def phase_started(self, **kwargs):\n",
    "        for cb in self.callbacks:\n",
    "            cb.phase_started(**kwargs)\n",
    "\n",
    "    def phase_ended(self, **kwargs):\n",
    "        for cb in self.callbacks:\n",
    "            cb.phase_ended(**kwargs)\n",
    "    \n",
    "    def epoch_started(self, **kwargs):\n",
    "        for cb in self.callbacks:\n",
    "            cb.epoch_started(**kwargs)\n",
    "    \n",
    "    def epoch_ended(self, **kwargs):\n",
    "        for cb in self.callbacks:\n",
    "            cb.epoch_ended(**kwargs)\n",
    "    \n",
    "    def batch_started(self, **kwargs):\n",
    "        for cb in self.callbacks:\n",
    "            cb.batch_started(**kwargs)\n",
    "            \n",
    "    def batch_ended(self, **kwargs):\n",
    "        for cb in self.callbacks:\n",
    "            cb.batch_ended(**kwargs)\n",
    "    \n",
    "    def before_forward_pass(self, **kwargs):\n",
    "        for cb in self.callbacks:\n",
    "            cb.before_forward_pass(**kwargs)\n",
    "    \n",
    "    def after_forward_pass(self, **kwargs):\n",
    "        for cb in self.callbacks:\n",
    "            cb.after_forward_pass(**kwargs)\n",
    "    \n",
    "    def before_backward_pass(self, **kwargs):\n",
    "        for cb in self.callbacks:\n",
    "            cb.before_forward_pass(**kwargs)\n",
    "    \n",
    "    def after_backward_pass(self, **kwargs):\n",
    "        for cb in self.callbacks:\n",
    "            cb.after_backward_pass(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Phase:\n",
    "    \"\"\"\n",
    "    Model training loop phase.\n",
    "\n",
    "    Each model's training loop iteration could be separated into (at least) two\n",
    "    phases: training and validation. The instances of this class track\n",
    "    metrics and counters, related to the specific phase, and keep the reference\n",
    "    to subset of data, used during phase.\n",
    "    \"\"\"\n",
    "    def __init__(self, name: str, loader: DataLoader, grad: bool=True):\n",
    "        self.name = name\n",
    "        self.loader = loader\n",
    "        self.grad = grad\n",
    "        self.batch_loss = None\n",
    "        self.batch_index = 0\n",
    "        self.rolling_loss = 0\n",
    "        self.losses = []\n",
    "        self.metrics = OrderedDict()\n",
    "        \n",
    "    @property\n",
    "    def last_loss(self):\n",
    "        return self.losses[-1] if self.losses else None\n",
    "    \n",
    "    @property\n",
    "    def last_metrics(self):\n",
    "        metrics = OrderedDict()\n",
    "        metrics[f'{self.name}_loss'] = self.last_loss\n",
    "        for name, values in self.metrics.items():\n",
    "            metrics[f'{self.name}_{name}'] = values[-1]\n",
    "        return metrics\n",
    "    \n",
    "    @property\n",
    "    def metrics_history(self):\n",
    "        metrics = OrderedDict()\n",
    "        for name, values in self.metrics.items():\n",
    "            metrics[f'{self.name}_{name}'] = values\n",
    "        return metrics\n",
    "\n",
    "    def update(self, loss):\n",
    "        self.losses.append(loss)\n",
    "        \n",
    "    def update_metric(self, name, value):\n",
    "        if name not in self.metrics:\n",
    "            self.metrics[name] = []\n",
    "        self.metrics[name].append(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExpandChannels:\n",
    "    \n",
    "    def __init__(self, num_of_channels=3):\n",
    "        self.nc = num_of_channels\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        return x.expand((self.nc,) + x.shape[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transforms():\n",
    "    return Compose([\n",
    "        ToTensor(), \n",
    "        ExpandChannels(3),\n",
    "        Normalize((0.1307,), (0.3081,))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def place_and_unwrap(batch, dev):\n",
    "    x, *y = batch\n",
    "    x = x.to(dev)\n",
    "    y = [tensor.to(dev) for tensor in y]\n",
    "    if len(y) == 1:\n",
    "        [y] = y\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "#         return F.log_softmax(x, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freeze_status(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec036676f8cb4082a4c1b11347c7b0e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='train', max=118), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb1f49b45e784642b6c838851acbe8ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='valid', max=20), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:    1 | train_loss=1.1271, train_accuracy=0.6068, valid_loss=0.8070, valid_accuracy=0.7388\n",
      "Epoch:    2 | train_loss=0.9265, train_accuracy=0.6950, valid_loss=0.7347, valid_accuracy=0.7761\n",
      "Epoch:    3 | train_loss=0.8236, train_accuracy=0.7300, valid_loss=0.6991, valid_accuracy=0.7827\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n = 4\n",
    "n_jobs = default(n, cpu_count())\n",
    "epochs = 3\n",
    "batch_size = 512\n",
    "device = torch.device('cuda:1')\n",
    "\n",
    "transforms = get_transforms()\n",
    "root = Path('~/data/mnist').expanduser()\n",
    "train_ds = MNIST(root, train=True, download=True, transform=transforms)\n",
    "valid_ds = MNIST(root, train=False, transform=transforms)\n",
    "\n",
    "phases = [\n",
    "    Phase('train', DataLoader(train_ds, batch_size, shuffle=True)),\n",
    "    Phase('valid', DataLoader(valid_ds, batch_size), grad=False)\n",
    "]\n",
    "\n",
    "# model = Net().to(device)\n",
    "# opt = optim.Adam(model.parameters(), lr=1e-2)\n",
    "\n",
    "model = Classifier(10, arch=models.resnet18).to(device)\n",
    "model.freeze_backbone()\n",
    "opt = optim.Adam(model.parameters(), lr=1e-2)\n",
    "\n",
    "# opt = optim.Adam([\n",
    "#     {'params': model.backbone.parameters(), 'lr': 1e-3},\n",
    "#     {'params': model.top.parameters(), 'lr': 1e-2}\n",
    "# ])\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "cb = CallbacksGroup([History(), \n",
    "                     Scheduler(\n",
    "                         CosineAnnealingSchedule(\n",
    "                             eta_min=1e-3, \n",
    "                             t_max=len(phases[0].loader)), \n",
    "                         mode='batch'),\n",
    "                     RollingLoss(), \n",
    "                     Accuracy(),\n",
    "                     StreamLogger(), \n",
    "                     ProgressBar()])\n",
    "cb.training_started(phases=phases, optimizer=opt)\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    cb.epoch_started(epoch=epoch)\n",
    "\n",
    "    for phase in phases:\n",
    "        n = len(phase.loader)\n",
    "        cb.phase_started(phase=phase, total_batches=n)\n",
    "        is_training = phase.grad\n",
    "        model.train(is_training)\n",
    "        \n",
    "        for batch in phase.loader:\n",
    "\n",
    "            phase.batch_index += 1\n",
    "            cb.batch_started(phase=phase, total_batches=n)\n",
    "            x, y = place_and_unwrap(batch, device)\n",
    "            \n",
    "            with torch.set_grad_enabled(is_training):\n",
    "                cb.before_forward_pass()\n",
    "                out = model(x)\n",
    "                cb.after_forward_pass()\n",
    "                loss = loss_fn(out, y)\n",
    "            \n",
    "            if is_training:\n",
    "                opt.zero_grad()\n",
    "                cb.before_backward_pass()\n",
    "                loss.backward()\n",
    "                cb.after_backward_pass()\n",
    "                opt.step()\n",
    "            \n",
    "            phase.batch_loss = loss.item()\n",
    "            cb.batch_ended(phase=phase, output=out, target=y)\n",
    "            \n",
    "        cb.phase_ended(phase=phase)\n",
    "    \n",
    "    cb.epoch_ended(phases=phases, epoch=epoch)\n",
    "    \n",
    "cb.training_ended(phases=phases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb['history'].plot(['train_loss', 'valid_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb['history'].plot(['train_accuracy', 'valid_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Classifier(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_status(layer):\n",
    "    params = list(layer.parameters())\n",
    "    if not params:\n",
    "        return 'no params'\n",
    "    trainable = [p for p in params if p.requires_grad]\n",
    "    return 'frozen' if not trainable else ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_status(m, stream=sys.stdout):\n",
    "    \"\"\"Flattens the model and prints its require_grad value.\"\"\"\n",
    "    stream.write('Layers status\\n')\n",
    "    stream.write('-' * 80 + '\\n')\n",
    "    for layer in flat_model(m):\n",
    "        name = layer.__class__.__name__\n",
    "        status = training_status(layer)\n",
    "        stream.write(f'{name:<68} [{status:9s}]\\n')\n",
    "    stream.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freeze_status(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def traverse(root, visit):\n",
    "    \n",
    "    def recurse(m, depth, index):\n",
    "        if depth >= 0:\n",
    "            visit(m, depth, index)\n",
    "        for index, child in enumerate(m.children()):\n",
    "            if hasattr(child, '__getitem__'):\n",
    "                recurse(child, depth+1, index)\n",
    "            else:\n",
    "                visit(child, depth+1, index)\n",
    "        \n",
    "    return recurse(root, -1, 0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
