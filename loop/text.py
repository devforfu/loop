# -----------------------------------------
# THIS FILE WAS AUTOGENERATED! DO NOT EDIT!
# -----------------------------------------
# file to edit: 05a_text.ipynb

"""
Text processing utils. Mostly copied from the fastai library.

The utilities help to convert "raw" texts into formats more suitable for
NLP models. The texts are cleaned and converted into list of tokens.
"""
from collections import Counter, OrderedDict
from itertools import chain
import html
from multiprocessing import cpu_count
from pathlib import Path
import re

from joblib import Parallel, delayed
import pandas as pd
import spacy
from spacy.lang.en import English

from loop.annotations import MaybeList, Callable
from loop.utils import combine, chunks


SEP    = 'â€¢'
T_UNK  = 'xxunk'
T_PAD  = 'xxpad'
T_BOS  = 'xxbos'
T_EOS  = 'xxeos'
T_REP  = 'xxrep'
T_WREP = 'xxwrep'
T_UP   = 'xxup'
T_MAJ  = 'xxmaj'
TOKENS = [T_UNK, T_PAD, T_BOS, T_EOS, T_REP, T_WREP, T_UP, T_MAJ]


def replace_tabs_with_spaces(s: str) -> str: return s.replace('\t', ' ')


def add_spaces_around(s: str) -> str: return re.sub(r'([/#\n])', r' \1 ', s)


def trim_useless_spaces(s: str) -> str: return re.sub(' {2,}', ' ', s)


def replace_repeated_chars(s: str) -> str:
    def _replace(match):
        char, repeats = match.groups()
        return f' {T_REP} {len(repeats) + 1} {char} '
    regex = re.compile(r'(\S)(\1{3,})')
    return regex.sub(_replace, s)


def replace_repeated_words(s: str) -> str:
    def _replace(match):
        word, repeats = match.groups()
        return f' {T_WREP} {len(repeats.split()) + 1} {word} '
    regex = re.compile(r'(\b\w+\W+)(\1{3,})')
    return regex.sub(_replace, s)


def replace_br_tags(s: str) -> str: return re.sub(r'<[\s]*br[\s]*/[\s]*>', '\n', s)


def fix_special_cases(s: str) -> str:
    regex = re.compile(r'  +')
    s = (s.
         replace('#39;',  "'").replace('amp;',    '&').replace('#146;',   "'").
         replace('nbsp;', ' ').replace('#36;',    '$').replace('\\n',    "\n").
         replace('quot;', "'").replace('\\"',     '"').replace(' @.@ ',   '.').
         replace(' @-@ ', '-').replace(' @,@ ',   ',').replace('\\',   ' \\ ').
         replace('<unk>', T_UNK))
    return regex.sub(' ', html.unescape(s))


def replace_new_lines(s: str) -> str: return s.replace('\n', ' ')


def replace_capslock(tokens: list) -> list:
    new = []
    for token in tokens:
        if token.isupper() and len(token) > 1:
            new += [T_UP, token.lower()]
        else:
            new.append(token)
    return new


def replace_capitalized(tokens: list) -> list:
    new = []
    for token in tokens:
        if token == '':
            continue
        if token[0].isupper() and len(token) > 1 and token[1:].islower():
            new.append(T_MAJ)
        new.append(token.lower())
    return new


PREP_RULES = [
    replace_tabs_with_spaces,
    replace_br_tags,
    fix_special_cases,
    replace_repeated_chars,
    replace_repeated_words,
    replace_new_lines,
    trim_useless_spaces
]

POST_RULES = [
    replace_capslock,
    replace_capitalized
]


def clean_text(s: str, rules=None):
    rules = rules or PREP_RULES
    return combine(s, *rules)


def update_tokens(tokens: str, rules=None):
    rules = rules or POST_RULES
    return combine(tokens, *rules)


def tokenize_english(texts: list):
    return tokenize(texts, chunk_size=100_000, num_workers=cpu_count(), special=TOKENS)


def tokenize(texts: list, chunk_size: int, num_workers: int=1,
             model_fn=English, prep=clean_text, post=update_tokens,
             special=None, backend='loky'):

    def doc_to_list(doc: str):
        return [token.text for token in doc]

    def worker(nlp, texts):
        return [post(doc_to_list(nlp.make_doc(prep(text)))) for text in texts]

    if len(texts) <= 2*chunk_size:
        nlp = init_tokenizer(model_fn, special)
        return worker(nlp, texts)

    with Parallel(n_jobs=num_workers, backend=backend) as parallel:
        results = parallel(
            delayed(worker)(nlp, text_chunk)
            for nlp, text_chunk in (
                (init_tokenizer(model_fn, special), t)
                for t in chunks(texts, chunk_size)
            )
        )

    return list(chain(*results))


def init_tokenizer(model_fn, special=None):
    nlp = model_fn()
    if special is not None:
        for t in special:
            nlp.tokenizer.add_special_case(t, [{spacy.symbols.ORTH: t}])
    return nlp


def useless_token(token, remove=('=', ' ')):
    return token in remove


def format_tokens(tokens): return SEP.join(tokens)


def print_tokens(tokens): print(format_tokens(tokens))


def read_files(root, labels=None, ext='txt', as_pandas=False):
    """Reads files from folders, using each one as a label name."""
    texts = []
    for path in Path(root).expanduser().iterdir():
        if path.is_dir():
            label = path.stem
            if labels is not None and label in labels:
                continue
            items = [
                {'text': fn.open().read(), 'name': fn.stem, 'label': label}
                for fn in path.glob(f'*.{ext}')]
            texts += items
    return pd.DataFrame(texts) if as_pandas else texts
